{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvxloP_xpeU"
      },
      "source": [
        "**INTERPOLATION DU FILTRE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les codes ci-dessous permettent d'abord d'interpoler le filtre de dimension 4x4 vers une dimension 50x50. \n",
        "Ensuite, le filtre est projeté dans l'espace de Schwartz. Explication dans le rapport."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGYfASukxtuA"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import RegularGridInterpolator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "# -----------------------------\n",
        "# 1. Interpolation du filtre\n",
        "# -----------------------------\n",
        "def interpolate_filter(filter_weights, smooth_factor=50):\n",
        "    \"\"\"Interpolation cubique bilinéaire pour passer du filtre discret à une fonction continue\n",
        "    retour : \n",
        "    - Zsmooth : filtre projeté en dimension 50x50\n",
        "    \"\"\"\n",
        "    h, w = filter_weights.shape\n",
        "    y = np.arange(h)\n",
        "    x = np.arange(w)\n",
        "    interp_func = RegularGridInterpolator((y, x), filter_weights, method='cubic')\n",
        "    ynew = np.linspace(0, h-1, smooth_factor)\n",
        "    xnew = np.linspace(0, w-1, smooth_factor)\n",
        "    X, Y = np.meshgrid(xnew, ynew)\n",
        "    points = np.array([Y.ravel(), X.ravel()]).T\n",
        "    Zsmooth = interp_func(points).reshape(smooth_factor, smooth_factor)\n",
        "    return Zsmooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA0CEZ2Vx3AF"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Projection dans l'espace de Schwartz\n",
        "# -----------------------------\n",
        "def project_to_schwartz(Zsmooth, sigma=None):\n",
        "    \"\"\"Projette un filtre interpolé dans un espace proche de S(R^2) via une fenêtre gaussienne\n",
        "    retour : \n",
        "    - Z_schwartz : filtre projeté dans l'espace de Schwartz \n",
        "    \"\"\"\n",
        "    H, W = Zsmooth.shape\n",
        "    if sigma is None:\n",
        "        sigma = min(H, W) / 2\n",
        "    y = np.arange(H) - H/2\n",
        "    x = np.arange(W) - W/2\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    gauss_window = np.exp(-(X**2 + Y**2) / (2*sigma**2))\n",
        "    Z_schwartz = Zsmooth * gauss_window\n",
        "    return Z_schwartz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8bfJzHt1uFQ"
      },
      "source": [
        "**NORME DE SOBOLEV : MESURE DE REGULARITE** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le code ci-dessous permet de définir la norme de Sobolev utilisé pour mesure la régularité des filtres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFS613_Wx7NN"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Définition de la fenêtre Gausienne utilisée pour la définition de la norme locale\n",
        "# -----------------------------\n",
        "def gaussian_window(shape, sigma=1.0, center=None):\n",
        "    \"\"\"Fenêtre gaussienne normalisée 2D\n",
        "    retour : \n",
        "    - g : la fenêtre gausienne\n",
        "    \"\"\"\n",
        "    n, m = shape\n",
        "    if center is None:\n",
        "        center = (n//2, m//2)\n",
        "    y = np.arange(n) - center[0]\n",
        "    x = np.arange(m) - center[1]\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    g = np.exp(-(X**2 + Y**2)/(2*sigma**2))\n",
        "    g /= np.sqrt(np.sum(g**2))\n",
        "    return g\n",
        "# -----------------------------\n",
        "# 2. Définition de la norme de Sobolev locale\n",
        "# -----------------------------\n",
        "def local_sobolev_gauss_3D(h, block_size=(5,5), alpha=2, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Calcule la régularité locale de sous-blocs d'un filtre 2D\n",
        "    avec fenêtre gaussienne et pondération Sobolev.\n",
        "\n",
        "    retour :\n",
        "    - reg_map : matrice de régularité locale\n",
        "    \"\"\"\n",
        "    n, m = h.shape\n",
        "    bh, bw = block_size\n",
        "    reg_map = np.zeros((n - bh + 1, m - bw + 1))\n",
        "\n",
        "    for i in range(n - bh + 1):\n",
        "        for j in range(m - bw + 1):\n",
        "            block = h[i:i+bh, j:j+bw]\n",
        "            g = gaussian_window(block.shape, sigma=sigma)\n",
        "            block_win = block * g\n",
        "\n",
        "            H = np.fft.fft2(block_win)\n",
        "            H = np.fft.fftshift(H)\n",
        "\n",
        "            u = np.fft.fftshift(np.fft.fftfreq(bh))\n",
        "            v = np.fft.fftshift(np.fft.fftfreq(bw))\n",
        "            U, V = np.meshgrid(u, v, indexing='ij')\n",
        "\n",
        "            freq_weight = (1 + U**2 + V**2)**alpha\n",
        "            reg_map[i,j] = np.sum(freq_weight * np.abs(H)**2)\n",
        "\n",
        "    return reg_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_7Fds61Q10"
      },
      "source": [
        "**TRAITEMENT DES POIDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois la norme définit, il nous définissons une methode de comparaison des filtres sur la base de cette dernière"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90sq5XIUxeBg"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Fonction de sélection (utiliser pour définir le P)\n",
        "# -----------------------------\n",
        "def graphcut_filter_surface(Zsmooth, percentile):\n",
        "    \"\"\"\n",
        "    Conserve uniquement les valeurs les plus petites (en valeur absolue)\n",
        "    selon un seuil basé sur le percentile d'un filtre donné\n",
        "    \n",
        "    retour : \n",
        "    - Zcut : filtre conservant seuleument les poids de régularité appartenant au percentile de régularité donné\n",
        "    - mask : masque crée suite au filtrage\n",
        "    \"\"\"\n",
        "    threshold = np.percentile(np.abs(Zsmooth), percentile)\n",
        "    mask = np.abs(Zsmooth) <= threshold  # True là où on garde\n",
        "    Zcut = np.where(mask, Zsmooth, np.nan)  # NaN pour visualiser les trous\n",
        "    return Zcut, mask\n",
        "def magnitude_mesure(Zcut):\n",
        "    \"\"\"\n",
        "    Calcule la somme des valeurs absolues de Zcut.\n",
        "    Les NaN sont traités comme des zéros.\n",
        "    \"\"\"\n",
        "    return np.nansum(np.abs(Zcut))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPbX5jqYDHnS"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Calcule de la régularité sur l'ensemble du filtre\n",
        "# -----------------------------\n",
        "def ultimate_graphcut_filter_analysis_reg(filter_weights):\n",
        "  \"\"\"\n",
        "  Calcule la régularité totale d'un filtre en se basant sur la mesure locale\n",
        "  \n",
        "  retour : \n",
        "  - valeur de la régularité du filtre\n",
        "  \"\"\"\n",
        "  Zsmooth = interpolate_filter(filter_weights)\n",
        "  Z_schwartz = project_to_schwartz(Zsmooth)\n",
        "  reg_map = local_sobolev_gauss_3D(Zsmooth, block_size=(5,5), alpha=2, sigma=58)\n",
        "  Zcut, mask = graphcut_filter_surface(reg_map, percentile=80) #Pourcentage à modifier \n",
        "  return magnitude_mesure(Zcut)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp20qDf12KYW"
      },
      "source": [
        "**CLASSEMENT DES FILTRES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois la norme établit, nous parcourons l'ensemble des filtres de l'ensemble des couches de convolution afin de classe les filtres selon la valeur de la norme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgwbKDtt6rmd",
        "outputId": "a11c6456-b498-4095-941a-70b4ea92f71d"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Classement des filtres selon leur valeur de régularité\n",
        "# -----------------------------\n",
        "conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n",
        "\n",
        "filter_ranking = {}\n",
        "conv_layers = [m for m in model.modules() if isinstance(m, torch.nn.Conv2d)]\n",
        "\n",
        "for m_idx, conv_layer in enumerate(conv_layers):\n",
        "    print(f\"   Couche {m_idx+1}/{len(conv_layers)}...\")\n",
        "\n",
        "    # Vérification si la couche est un bottleneck\n",
        "    # Si la couche est un bottleneck, on la saute\n",
        "    # Le bottleneck est une convolution 1x1 après une couche de plus grande taille, on suppose que c'est un critère ici\n",
        "    if conv_layer.kernel_size == (1, 1):\n",
        "        print(f\"   Couche {m_idx+1} est un bottleneck, on passe.\")\n",
        "        continue\n",
        "\n",
        "    filter_ranking[m_idx] = {}\n",
        "\n",
        "    out_channels, in_channels, h, w = conv_layer.weight.shape\n",
        "\n",
        "    for oc in range(out_channels):\n",
        "        print(oc)\n",
        "        filter_ranking[m_idx][oc] = {}\n",
        "        for ic in range(in_channels):\n",
        "            print(ic)\n",
        "\n",
        "            # 1. Récupère les poids du filtre\n",
        "            filter_weights = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "\n",
        "            #2. Calcule la régularité du filtre\n",
        "            base_importance = ultimate_graphcut_filter_analysis_reg(filter_weights)\n",
        "\n",
        "            # 3. Peuple le dictionnaire à l'indice de la couche et du filtre correspondant, la valeur étant la mesure de régularité\n",
        "            filter_ranking[m_idx][oc][ic] = float(base_importance)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Enregistrement du dictionnaire\n",
        "# -----------------------------\n",
        "import json \n",
        "with open(\"filter_ranking.json\", \"w\") as f: \n",
        "    json.dump(filter_ranking, f, indent=4)\n",
        "from google.colab import files\n",
        "files.download(\"filter_ranking.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PRUNING DU MODELE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJg-VzejFbTq"
      },
      "source": [
        "Une fois le dictionnaire de classement des filtres récupéré, nous prunons notre modèle en conservant un pourcentage fixe des filtres les plus réguliers. Nous reconstruisons ainsi un modèle plus léger que nous ré-entrainons ensuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keep_top_percentile_filters(filter_ranking, percentile):\n",
        "    \"\"\"\n",
        "    Keeps the top 'percentile' of filters based on their base_importance score.\n",
        "\n",
        "    Args:\n",
        "        filter_ranking (dict): A dictionary containing the ranking of filters.\n",
        "                               Expected format: {layer_idx: {out_channel_idx: {in_channel_idx: base_importance}}}\n",
        "        percentile (int): The percentile to keep (e.g., 10 for top 10%).\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary containing only the top percentile filters.\n",
        "    \"\"\"\n",
        "    all_importances = []\n",
        "    # Collect all base_importance values\n",
        "    for layer_idx, out_channels in filter_ranking.items():\n",
        "        for out_channel_idx, in_channels in out_channels.items():\n",
        "            for in_channel_idx, importance in in_channels.items():\n",
        "                # Only consider non-NaN importance values\n",
        "                if not np.isnan(importance):\n",
        "                    all_importances.append(importance)\n",
        "\n",
        "    if not all_importances:\n",
        "        print(\"No valid importance values found.\")\n",
        "        return {}\n",
        "\n",
        "    # Calculate the threshold based on the percentile\n",
        "    threshold = np.percentile(all_importances, 100 - percentile) # Keep values >= threshold\n",
        "\n",
        "    # Create a new dictionary with only the top percentile filters\n",
        "    top_filters_ranking = {}\n",
        "    for layer_idx, out_channels in filter_ranking.items():\n",
        "        top_filters_ranking[layer_idx] = {}\n",
        "        for out_channel_idx, in_channels in out_channels.items():\n",
        "            top_filters_ranking[layer_idx][out_channel_idx] = {}\n",
        "            for in_channel_idx, importance in in_channels.items():\n",
        "                if not np.isnan(importance) and importance <= threshold:\n",
        "                    top_filters_ranking[layer_idx][out_channel_idx][in_channel_idx] = importance\n",
        "\n",
        "    return top_filters_ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf8xk3vXFuog"
      },
      "outputs": [],
      "source": [
        "def structured_channel_pruning(model, importance_dict, keep_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Pruning structuré basé sur la régularité (petites valeurs = plus régulier = garder).\n",
        "\n",
        "    Returns:\n",
        "        dict[layer_idx] = [list of output channels to keep]\n",
        "    \"\"\"\n",
        "    print(f\"\\n  DEBUG: importance_dict a {len(importance_dict)} couches\")\n",
        "\n",
        "    channel_importance = {}\n",
        "\n",
        "    for layer_idx in importance_dict:\n",
        "        channel_importance[layer_idx] = {}\n",
        "        for out_ch in importance_dict[layer_idx]:\n",
        "            avg_regularity = np.mean(list(importance_dict[layer_idx][out_ch].values()))\n",
        "            channel_importance[layer_idx][out_ch] = avg_regularity\n",
        "\n",
        "    channels_to_keep = {}\n",
        "\n",
        "    for layer_idx in channel_importance:\n",
        "        # Tri croissant : plus petites valeurs (plus régulier) en premier\n",
        "        channels_by_regularity = sorted(\n",
        "            channel_importance[layer_idx].items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=False\n",
        "        )\n",
        "\n",
        "        total_channels = len(channels_by_regularity)\n",
        "        n_keep = max(1, int(total_channels * keep_ratio))\n",
        "\n",
        "        \n",
        "        kept_channel_indices = sorted([int(ch) for ch, _ in channels_by_regularity[:n_keep]])\n",
        "\n",
        "       \n",
        "        channels_to_keep[int(layer_idx)] = kept_channel_indices\n",
        "\n",
        "        \n",
        "        print(f\"  Layer {int(layer_idx)}: garde {n_keep}/{total_channels} canaux -> {kept_channel_indices[:5]}{'...' if len(kept_channel_indices) > 5 else ''}\")\n",
        "\n",
        "        \n",
        "        if n_keep == total_channels:\n",
        "            print(f\"    ⚠️  ATTENTION: On garde 100% des canaux de la couche {layer_idx}!\")\n",
        "\n",
        "    return channels_to_keep\n",
        "\n",
        "\n",
        "def calculate_output_size_after_layer(layer, h, w, channels):\n",
        "    \"\"\"Calcule la taille de sortie après une couche\"\"\"\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        kernel_size = layer.kernel_size[0] if isinstance(layer.kernel_size, tuple) else layer.kernel_size\n",
        "        stride = layer.stride[0] if isinstance(layer.stride, tuple) else layer.stride\n",
        "        padding = layer.padding[0] if isinstance(layer.padding, tuple) else layer.padding\n",
        "\n",
        "        h = (h + 2*padding - kernel_size) // stride + 1\n",
        "        w = (w + 2*padding - kernel_size) // stride + 1\n",
        "        channels = layer.out_channels\n",
        "\n",
        "    elif isinstance(layer, nn.MaxPool2d):\n",
        "        kernel_size = layer.kernel_size\n",
        "        stride = layer.stride if layer.stride else kernel_size\n",
        "        h = h // stride\n",
        "        w = w // stride\n",
        "\n",
        "    elif isinstance(layer, nn.AdaptiveAvgPool2d):\n",
        "        # AdaptiveAvgPool force la sortie à une taille fixe\n",
        "        if isinstance(layer.output_size, tuple):\n",
        "            h, w = layer.output_size\n",
        "        else:\n",
        "            h = w = layer.output_size\n",
        "\n",
        "    return h, w, channels\n",
        "\n",
        "\n",
        "def prune_model_by_channel_removal(model, channels_to_keep, input_size=(3, 32, 32)):\n",
        "    \"\"\"\n",
        "    Supprime des canaux entiers du modèle.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model_copy = copy.deepcopy(model)\n",
        "\n",
        "    print(f\"\\n  DEBUG prune_model: channels_to_keep = {channels_to_keep}\")\n",
        "\n",
        "    new_layers = []\n",
        "    conv_layers = [i for i, layer in enumerate(model_copy) if isinstance(layer, nn.Conv2d)]\n",
        "\n",
        "    print(f\"  Couches Conv trouvées aux indices: {conv_layers}\")\n",
        "\n",
        "    prev_kept_channels = None\n",
        "\n",
        "    for i, layer in enumerate(model_copy):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            conv_idx = conv_layers.index(i)\n",
        "\n",
        "            # Canaux de sortie à garder\n",
        "            keep_out_channels = channels_to_keep.get(conv_idx, list(range(layer.out_channels)))\n",
        "\n",
        "            print(f\"  Conv {conv_idx} (indice {i}): {layer.out_channels} canaux -> garde {len(keep_out_channels)} canaux\")\n",
        "\n",
        "            # Canaux d'entrée\n",
        "            if conv_idx == 0:\n",
        "                keep_in_channels = list(range(layer.in_channels))\n",
        "            else:\n",
        "                keep_in_channels = prev_kept_channels if prev_kept_channels else list(range(layer.in_channels))\n",
        "\n",
        "            # Créer nouvelle couche\n",
        "            new_conv = nn.Conv2d(\n",
        "                in_channels=len(keep_in_channels),\n",
        "                out_channels=len(keep_out_channels),\n",
        "                kernel_size=layer.kernel_size,\n",
        "                stride=layer.stride,\n",
        "                padding=layer.padding,\n",
        "                bias=(layer.bias is not None)\n",
        "            ).to(device)\n",
        "\n",
        "            print(f\"    Nouvelle Conv: in={len(keep_in_channels)}, out={len(keep_out_channels)}\")\n",
        "\n",
        "            # Copier les poids\n",
        "            with torch.no_grad():\n",
        "                for new_out_idx, old_out_idx in enumerate(keep_out_channels):\n",
        "                    for new_in_idx, old_in_idx in enumerate(keep_in_channels):\n",
        "                        new_conv.weight[new_out_idx, new_in_idx] = layer.weight[old_out_idx, old_in_idx]\n",
        "                    if layer.bias is not None:\n",
        "                        new_conv.bias[new_out_idx] = layer.bias[old_out_idx]\n",
        "\n",
        "            new_layers.append(new_conv)\n",
        "            prev_kept_channels = keep_out_channels\n",
        "\n",
        "            # ✅ Vérification: s'assurer que la réduction a bien eu lieu\n",
        "            if len(keep_out_channels) == layer.out_channels:\n",
        "                print(f\"    ⚠️  AUCUNE réduction pour Conv {conv_idx}!\")\n",
        "            else:\n",
        "                reduction = (1 - len(keep_out_channels) / layer.out_channels) * 100\n",
        "                print(f\"    ✅ Réduction de {reduction:.1f}% pour Conv {conv_idx}\")\n",
        "\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            if prev_kept_channels is not None:\n",
        "                new_bn = nn.BatchNorm2d(len(prev_kept_channels)).to(device)\n",
        "                with torch.no_grad():\n",
        "                    for new_idx, old_idx in enumerate(prev_kept_channels):\n",
        "                        new_bn.weight[new_idx] = layer.weight[old_idx]\n",
        "                        new_bn.bias[new_idx] = layer.bias[old_idx]\n",
        "                        new_bn.running_mean[new_idx] = layer.running_mean[old_idx]\n",
        "                        new_bn.running_var[new_idx] = layer.running_var[old_idx]\n",
        "                new_layers.append(new_bn)\n",
        "            else:\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            # Vérifier si c'est la première Linear après les conv\n",
        "            if len([l for l in new_layers if isinstance(l, nn.Linear)]) == 0:\n",
        "                # Calculer la taille réelle après toutes les couches conv\n",
        "                channels, h, w = input_size\n",
        "\n",
        "                for prev_layer in new_layers:\n",
        "                    h, w, channels = calculate_output_size_after_layer(prev_layer, h, w, channels)\n",
        "\n",
        "                new_input_features = channels * h * w\n",
        "\n",
        "                print(f\"  Linear input: {new_input_features} (channels={channels}, h={h}, w={w})\")\n",
        "\n",
        "                new_linear = nn.Linear(\n",
        "                    in_features=new_input_features,\n",
        "                    out_features=layer.out_features,\n",
        "                    bias=(layer.bias is not None)\n",
        "                ).to(device)\n",
        "\n",
        "                # Initialiser aléatoirement (pas de copie des poids car taille incompatible)\n",
        "                with torch.no_grad():\n",
        "                    nn.init.kaiming_normal_(new_linear.weight)\n",
        "                    if layer.bias is not None:\n",
        "                        new_linear.bias.zero_()\n",
        "\n",
        "                new_layers.append(new_linear)\n",
        "            else:\n",
        "                # Autres Linear: copier tel quel\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "        else:\n",
        "            # Autres couches (ReLU, MaxPool, Dropout, Flatten, AdaptiveAvgPool)\n",
        "            new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "    return nn.Sequential(*new_layers).to(device)\n",
        "\n",
        "\n",
        "def test_effective_pruning(model, importance_dict, trn_dl, val_dl, loss_fn,\n",
        "                          keep_ratios=[0.9, 0.8, 0.7, 0.6, 0.5],\n",
        "                          return_best=True):\n",
        "    \"\"\"\n",
        "    Test de pruning avec réduction effective des paramètres.\n",
        "\n",
        "    Args:\n",
        "        return_best: Si True, retourne le meilleur modèle (meilleur compromis accuracy/compression)\n",
        "\n",
        "    Returns:\n",
        "        Si return_best=True: (best_model, results)\n",
        "        Sinon: results\n",
        "    \"\"\"\n",
        "    print(\"Test de Pruning avec Réduction Effective des Paramètres\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Baseline\n",
        "    orig_params = sum(p.numel() for p in model.parameters())\n",
        "    orig_loss, orig_acc = evaluate_model(model, val_dl, loss_fn)\n",
        "\n",
        "    print(f\"Modèle original: {orig_acc:.4f} accuracy, {orig_params:,} paramètres\\n\")\n",
        "\n",
        "    results = []\n",
        "    best_model = None\n",
        "    best_score = -float('inf')\n",
        "\n",
        "    for keep_ratio in keep_ratios:\n",
        "        print(f\"Test avec keep_ratio = {keep_ratio} ({keep_ratio*100:.0f}% des canaux)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Pruning structuré\n",
        "            channels_to_keep = structured_channel_pruning(model, importance_dict, keep_ratio)\n",
        "\n",
        "            # Afficher le nombre de canaux gardés par couche\n",
        "            total_kept = sum(len(chs) for chs in channels_to_keep.values())\n",
        "            print(f\"  Total canaux gardés: {total_kept}\")\n",
        "\n",
        "            # Appliquer le pruning\n",
        "            pruned_model = prune_model_by_channel_removal(model, channels_to_keep, input_size=(3, 32, 32))\n",
        "\n",
        "            # Vérifier la réduction\n",
        "            pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
        "            actual_reduction = (orig_params - pruned_params) / orig_params * 100\n",
        "\n",
        "            print(f\"\\nParamètres avant: {orig_params:,}\")\n",
        "            print(f\"Paramètres après: {pruned_params:,}\")\n",
        "            print(f\"Réduction RÉELLE: {actual_reduction:.1f}%\")\n",
        "\n",
        "            # Test avant fine-tuning\n",
        "            val_loss_before, val_acc_before = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "            print(f\"Accuracy avant fine-tuning: {val_acc_before:.4f}\")\n",
        "\n",
        "            # Fine-tuning\n",
        "            print(\"Fine-tuning...\")\n",
        "            pruned_model = fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=5, lr=1e-3)\n",
        "\n",
        "            val_loss_final, val_acc_final = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "            accuracy_retention = (val_acc_final / orig_acc) * 100\n",
        "            compression_ratio = orig_params / pruned_params if pruned_params > 0 else 0\n",
        "\n",
        "            print(f\"Accuracy finale: {val_acc_final:.4f}\")\n",
        "            print(f\"Rétention accuracy: {accuracy_retention:.1f}%\")\n",
        "            print(f\"Ratio de compression: {compression_ratio:.2f}x\\n\")\n",
        "\n",
        "            # Calculer un score pour trouver le meilleur modèle\n",
        "            # Score = accuracy_retention * log(compression_ratio)\n",
        "            # Favorise un bon équilibre entre accuracy et compression\n",
        "            import math\n",
        "            score = accuracy_retention * math.log(compression_ratio + 1)\n",
        "\n",
        "            results.append({\n",
        "                'keep_ratio': keep_ratio,\n",
        "                'actual_reduction': actual_reduction,\n",
        "                'final_accuracy': val_acc_final,\n",
        "                'accuracy_retention': accuracy_retention,\n",
        "                'compression_ratio': compression_ratio,\n",
        "                'pruned_params': pruned_params,\n",
        "                'model': pruned_model,  # ✅ Sauvegarder le modèle\n",
        "                'score': score\n",
        "            })\n",
        "\n",
        "            # Garder le meilleur modèle\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_model = pruned_model\n",
        "                print(f\"    🏆 Nouveau meilleur modèle! (score: {score:.2f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"Erreur: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print()\n",
        "\n",
        "    # Résumé\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RÉSUMÉ DES RÉSULTATS\")\n",
        "        print(\"=\"*60)\n",
        "        for r in results:\n",
        "            marker = \"🏆\" if r['model'] == best_model else \"  \"\n",
        "            print(f\"{marker} Keep {r['keep_ratio']*100:.0f}%: {r['actual_reduction']:.1f}% réduction, \"\n",
        "                  f\"{r['accuracy_retention']:.1f}% accuracy, {r['compression_ratio']:.2f}x compression\")\n",
        "\n",
        "    if return_best:\n",
        "        return best_model, results\n",
        "    else:\n",
        "        return results\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_dl, loss_fn):\n",
        "    \"\"\"Évalue le modèle\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            device = next(model.parameters()).device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            total_loss += loss_fn(pred, y).item() * x.size(0)\n",
        "            total_correct += (pred.argmax(1) == y).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "\n",
        "def fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=5, lr=1e-3):\n",
        "    \"\"\"Fine-tune le modèle pruné\"\"\"\n",
        "    from torch.optim import Adam\n",
        "\n",
        "    device = next(pruned_model.parameters()).device\n",
        "    optimizer = Adam(pruned_model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pruned_model.train()\n",
        "        for x, y in trn_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = pruned_model(x)\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "        if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    return pruned_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyvjgo6gajdQ",
        "outputId": "798fec4d-c6ef-47af-a292-f6bb73c17803"
      },
      "outputs": [],
      "source": [
        "model_test = copy.deepcopy(model) \n",
        "pruned_model = test_effective_pruning(model_test,filter_ranking,train_loader, val_loader, loss_fn) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
