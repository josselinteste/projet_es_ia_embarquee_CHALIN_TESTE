{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz7un4mk6EPO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Data augmentation\n",
        "# -----------------------------\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.RandomErasing(p=0.2)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Importation des données de CIPHAR-10\n",
        "# -----------------------------\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Définition du modèle\n",
        "# -----------------------------\n",
        "def get_model_improved(device='cpu'):\n",
        "    model = nn.Sequential(\n",
        "    # Bloc 1\n",
        "    nn.Conv2d(3, 32, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "\n",
        "    nn.Conv2d(32, 32, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Dropout2d(0.2),\n",
        "\n",
        "    # Bloc 2\n",
        "    nn.Conv2d(32, 64, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Dropout2d(0.3),\n",
        "\n",
        "    # Bloc 3\n",
        "    nn.Conv2d(64, 128, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128),\n",
        "\n",
        "    nn.Conv2d(128, 128, kernel_size=1, padding=0, bias=True),  # bottleneck\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.AdaptiveAvgPool2d((1,1)),\n",
        "\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # Classifier compact\n",
        "    nn.Linear(128, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(128, 10)\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    return model, loss_fn, optimizer, scheduler\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Entraînement + Early stopping + Tracking\n",
        "# -----------------------------\n",
        "def train_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, device='cpu', epochs=30, patience=5):\n",
        "    best_val_loss = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Historique pour les courbes\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = loss_fn(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        train_loss = running_loss / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = loss_fn(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        val_loss /= total\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Enregistrement\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
        "              f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}% | \"\n",
        "              f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss > best_val_loss - 1e-3:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "        else:\n",
        "            patience_counter = 0\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. Affichage des courbes\n",
        "    # -----------------------------\n",
        "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss évolution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Val Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy ')\n",
        "    plt.title('Accuracy évolution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Lancement\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, loss_fn, optimizer, scheduler = get_model_improved(device)\n",
        "train_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Une fois le modèle entrainé, sauvegarde de ses poids\n",
        "torch.save(model.state_dict(), \"model_weights.pth\")\n",
        "files.download(\"model_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvxloP_xpeU"
      },
      "source": [
        "**INTERPOLATION DU FILTRE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les codes ci-dessous permettent d'abord d'interpoler le filtre de dimension 4x4 vers une dimension 50x50. \n",
        "Ensuite, le filtre est projeté dans l'espace de Schwartz. Explication dans le rapport."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGYfASukxtuA"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import RegularGridInterpolator\n",
        "# -----------------------------\n",
        "# 1. Interpolation du filtre\n",
        "# -----------------------------\n",
        "def interpolate_filter(filter_weights, smooth_factor=50):\n",
        "    \"\"\"Interpolation cubique bilinéaire pour passer du filtre discret à une fonction continue\n",
        "    retour : \n",
        "    - Zsmooth : filtre projeté en dimension 50x50\n",
        "    \"\"\"\n",
        "    h, w = filter_weights.shape\n",
        "    y = np.arange(h)\n",
        "    x = np.arange(w)\n",
        "    interp_func = RegularGridInterpolator((y, x), filter_weights, method='cubic')\n",
        "    ynew = np.linspace(0, h-1, smooth_factor)\n",
        "    xnew = np.linspace(0, w-1, smooth_factor)\n",
        "    X, Y = np.meshgrid(xnew, ynew)\n",
        "    points = np.array([Y.ravel(), X.ravel()]).T\n",
        "    Zsmooth = interp_func(points).reshape(smooth_factor, smooth_factor)\n",
        "    return Zsmooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA0CEZ2Vx3AF"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Projection dans l'espace de Schwartz\n",
        "# -----------------------------\n",
        "def project_to_schwartz(Zsmooth, sigma=None):\n",
        "    \"\"\"Projette un filtre interpolé dans un espace proche de S(R^2) via une fenêtre gaussienne\n",
        "    retour : \n",
        "    - Z_schwartz : filtre projeté dans l'espace de Schwartz \n",
        "    \"\"\"\n",
        "    H, W = Zsmooth.shape\n",
        "    if sigma is None:\n",
        "        sigma = min(H, W) / 2\n",
        "    y = np.arange(H) - H/2\n",
        "    x = np.arange(W) - W/2\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    gauss_window = np.exp(-(X**2 + Y**2) / (2*sigma**2))\n",
        "    Z_schwartz = Zsmooth * gauss_window\n",
        "    return Z_schwartz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8bfJzHt1uFQ"
      },
      "source": [
        "**NORME DE SOBOLEV : MESURE DE REGULARITE** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le code ci-dessous permet de définir la norme de Sobolev utilisé pour mesure la régularité des filtres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFS613_Wx7NN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "# -----------------------------\n",
        "# 1. Définition de la fenêtre Gausienne utilisée pour la définition de la norme locale\n",
        "# -----------------------------\n",
        "def gaussian_window(shape, sigma=1.0, center=None):\n",
        "    \"\"\"Fenêtre gaussienne normalisée 2D\n",
        "    retour : \n",
        "    - g : la fenêtre gausienne\n",
        "    \"\"\"\n",
        "    n, m = shape\n",
        "    if center is None:\n",
        "        center = (n//2, m//2)\n",
        "    y = np.arange(n) - center[0]\n",
        "    x = np.arange(m) - center[1]\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    g = np.exp(-(X**2 + Y**2)/(2*sigma**2))\n",
        "    g /= np.sqrt(np.sum(g**2))\n",
        "    return g\n",
        "# -----------------------------\n",
        "# 2. Définition de la norme de Sobolev locale\n",
        "# -----------------------------\n",
        "def local_sobolev_gauss_3D(h, block_size=(5,5), alpha=2, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Calcule la régularité locale de sous-blocs d'un filtre 2D\n",
        "    avec fenêtre gaussienne et pondération Sobolev.\n",
        "\n",
        "    retour :\n",
        "    - reg_map : matrice de régularité locale\n",
        "    \"\"\"\n",
        "    n, m = h.shape\n",
        "    bh, bw = block_size\n",
        "    reg_map = np.zeros((n - bh + 1, m - bw + 1))\n",
        "\n",
        "    for i in range(n - bh + 1):\n",
        "        for j in range(m - bw + 1):\n",
        "            block = h[i:i+bh, j:j+bw]\n",
        "            g = gaussian_window(block.shape, sigma=sigma)\n",
        "            block_win = block * g\n",
        "\n",
        "            H = np.fft.fft2(block_win)\n",
        "            H = np.fft.fftshift(H)\n",
        "\n",
        "            u = np.fft.fftshift(np.fft.fftfreq(bh))\n",
        "            v = np.fft.fftshift(np.fft.fftfreq(bw))\n",
        "            U, V = np.meshgrid(u, v, indexing='ij')\n",
        "\n",
        "            freq_weight = (1 + U**2 + V**2)**alpha\n",
        "            reg_map[i,j] = np.sum(freq_weight * np.abs(H)**2)\n",
        "\n",
        "    return reg_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_7Fds61Q10"
      },
      "source": [
        "**TRAITEMENT DES POIDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois la norme définit, il nous définissons une methode de comparaison des filtres sur la base de cette dernière"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90sq5XIUxeBg"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Fonction de sélection (utiliser pour définir le P)\n",
        "# -----------------------------\n",
        "def graphcut_filter_surface(Zsmooth, percentile):\n",
        "    \"\"\"\n",
        "    Conserve uniquement les valeurs les plus petites (en valeur absolue)\n",
        "    selon un seuil basé sur le percentile d'un filtre donné\n",
        "    \n",
        "    retour : \n",
        "    - Zcut : filtre conservant seuleument les poids de régularité appartenant au percentile de régularité donné\n",
        "    - mask : masque crée suite au filtrage\n",
        "    \"\"\"\n",
        "    threshold = np.percentile(np.abs(Zsmooth), percentile)\n",
        "    mask = np.abs(Zsmooth) <= threshold  # True là où on garde\n",
        "    Zcut = np.where(mask, Zsmooth, np.nan)  # NaN pour visualiser les trous\n",
        "    return Zcut, mask\n",
        "def magnitude_mesure(Zcut):\n",
        "    \"\"\"\n",
        "    Calcule la somme des valeurs absolues de Zcut.\n",
        "    Les NaN sont traités comme des zéros.\n",
        "    \"\"\"\n",
        "    return np.nansum(np.abs(Zcut))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPbX5jqYDHnS"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "# -----------------------------\n",
        "# 2. Calcule de la régularité sur l'ensemble du filtre\n",
        "# -----------------------------\n",
        "def ultimate_graphcut_filter_analysis_reg(filter_weights):\n",
        "  \"\"\"\n",
        "  Calcule la régularité totale d'un filtre en se basant sur la mesure locale\n",
        "  \n",
        "  retour : \n",
        "  - valeur de la régularité du filtre\n",
        "  \"\"\"\n",
        "  Zsmooth = interpolate_filter(filter_weights)\n",
        "  Z_schwartz = project_to_schwartz(Zsmooth)\n",
        "  reg_map = local_sobolev_gauss_3D(Zsmooth, block_size=(5,5), alpha=2, sigma=58)\n",
        "  Zcut, mask = graphcut_filter_surface(reg_map, percentile=80) #Pourcentage à modifier \n",
        "  return magnitude_mesure(Zcut)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp20qDf12KYW"
      },
      "source": [
        "**CLASSEMENT DES FILTRES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois la norme établit, nous parcourons l'ensemble des filtres de l'ensemble des couches de convolution afin de classe les filtres selon la valeur de la norme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgwbKDtt6rmd",
        "outputId": "a11c6456-b498-4095-941a-70b4ea92f71d"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import RegularGridInterpolator\n",
        "import torch\n",
        "# -----------------------------\n",
        "# 1. Classement des filtres selon leur valeur de régularité\n",
        "# -----------------------------\n",
        "conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n",
        "\n",
        "filter_ranking = {}\n",
        "conv_layers = [m for m in model.modules() if isinstance(m, torch.nn.Conv2d)]\n",
        "\n",
        "for m_idx, conv_layer in enumerate(conv_layers):\n",
        "    print(f\"   Couche {m_idx+1}/{len(conv_layers)}...\")\n",
        "\n",
        "    # Vérification si la couche est un bottleneck\n",
        "    # Si la couche est un bottleneck, on la saute\n",
        "    # Le bottleneck est une convolution 1x1 après une couche de plus grande taille, on suppose que c'est un critère ici\n",
        "    if conv_layer.kernel_size == (1, 1):\n",
        "        print(f\"   Couche {m_idx+1} est un bottleneck, on passe.\")\n",
        "        continue\n",
        "\n",
        "    filter_ranking[m_idx] = {}\n",
        "\n",
        "    out_channels, in_channels, h, w = conv_layer.weight.shape\n",
        "\n",
        "    for oc in range(out_channels):\n",
        "        print(oc)\n",
        "        filter_ranking[m_idx][oc] = {}\n",
        "        for ic in range(in_channels):\n",
        "            print(ic)\n",
        "\n",
        "            # 1. Récupère les poids du filtre\n",
        "            filter_weights = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "\n",
        "            #2. Calcule la régularité du filtre\n",
        "            base_importance = ultimate_graphcut_filter_analysis_reg(filter_weights)\n",
        "\n",
        "            # 3. Peuple le dictionnaire à l'indice de la couche et du filtre correspondant, la valeur étant la mesure de régularité\n",
        "            filter_ranking[m_idx][oc][ic] = float(base_importance)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Enregistrement du dictionnaire\n",
        "# -----------------------------\n",
        "import json \n",
        "with open(\"filter_ranking.json\", \"w\") as f: \n",
        "    json.dump(filter_ranking, f, indent=4)\n",
        "from google.colab import files\n",
        "files.download(\"filter_ranking.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PRUNING DU MODELE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJg-VzejFbTq"
      },
      "source": [
        "Une fois le dictionnaire de classement des filtres récupéré, nous prunons notre modèle en conservant un pourcentage fixe des filtres les plus réguliers. Nous reconstruisons ainsi un modèle plus léger que nous ré-entrainons ensuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf8xk3vXFuog"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def prune_model_by_filters_removal(model, top_indices_list, input_size=(3, 32, 32)):\n",
        "    \"\"\"\n",
        "    Supprime les filtres pour réduire les paramètres.\n",
        "\n",
        "    Args:\n",
        "        model: modèle PyTorch nn.Sequential\n",
        "        top_indices_list: [(layer_idx, out_ch, in_ch), ...] - filtres à CONSERVER\n",
        "        input_size: taille d'entrée (channels, height, width)\n",
        "\n",
        "    Returns:\n",
        "        Modèle pruné avec moins de paramètres\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model_copy = copy.deepcopy(model)\n",
        "\n",
        "    # Organiser les filtres à conserver par couche\n",
        "    filters_to_keep = defaultdict(set)\n",
        "    for layer_idx, out_ch, in_ch in top_indices_list:\n",
        "        filters_to_keep[layer_idx].add(out_ch)\n",
        "\n",
        "    # Convertir en listes triées\n",
        "    for layer_idx in filters_to_keep:\n",
        "        filters_to_keep[layer_idx] = sorted(list(filters_to_keep[layer_idx]))\n",
        "\n",
        "    new_layers = []\n",
        "    conv_layers = [i for i, layer in enumerate(model_copy) if isinstance(layer, nn.Conv2d)]\n",
        "    prev_kept_channels = None\n",
        "\n",
        "    for i, layer in enumerate(model_copy):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Trouver l'index de cette couche conv\n",
        "            conv_idx = conv_layers.index(i)\n",
        "\n",
        "            # Déterminer les canaux de sortie à conserver\n",
        "            if conv_idx in filters_to_keep:\n",
        "                keep_out_channels = filters_to_keep[conv_idx]\n",
        "            else:\n",
        "                # Si pas spécifié, garder tous les canaux\n",
        "                keep_out_channels = list(range(layer.out_channels))\n",
        "\n",
        "            # Déterminer les canaux d'entrée\n",
        "            if conv_idx == 0:\n",
        "                # Première couche: garder tous les canaux d'entrée (RGB)\n",
        "                new_in_channels = layer.in_channels\n",
        "                keep_in_channels = list(range(layer.in_channels))\n",
        "            else:\n",
        "                # Couches suivantes: s'adapter aux canaux conservés de la couche précédente\n",
        "                new_in_channels = len(prev_kept_channels) if prev_kept_channels else layer.in_channels\n",
        "                keep_in_channels = prev_kept_channels if prev_kept_channels else list(range(layer.in_channels))\n",
        "\n",
        "            new_out_channels = len(keep_out_channels)\n",
        "\n",
        "            # Créer la nouvelle couche Conv2d RÉDUITE\n",
        "            new_conv = nn.Conv2d(\n",
        "                in_channels=new_in_channels,\n",
        "                out_channels=new_out_channels,\n",
        "                kernel_size=layer.kernel_size,\n",
        "                stride=layer.stride,\n",
        "                padding=layer.padding,\n",
        "                bias=(layer.bias is not None)\n",
        "            ).to(device)\n",
        "\n",
        "            # Copier uniquement les poids des filtres conservés\n",
        "            with torch.no_grad():\n",
        "                for new_out_idx, old_out_idx in enumerate(keep_out_channels):\n",
        "                    for new_in_idx, old_in_idx in enumerate(keep_in_channels):\n",
        "                        new_conv.weight[new_out_idx, new_in_idx] = layer.weight[old_out_idx, old_in_idx]\n",
        "\n",
        "                    if layer.bias is not None:\n",
        "                        new_conv.bias[new_out_idx] = layer.bias[old_out_idx]\n",
        "\n",
        "            new_layers.append(new_conv)\n",
        "            prev_kept_channels = keep_out_channels\n",
        "\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            # Adapter BatchNorm aux canaux conservés\n",
        "            if prev_kept_channels is not None:\n",
        "                new_bn = nn.BatchNorm2d(len(prev_kept_channels)).to(device)\n",
        "                with torch.no_grad():\n",
        "                    for new_idx, old_idx in enumerate(prev_kept_channels):\n",
        "                        new_bn.weight[new_idx] = layer.weight[old_idx]\n",
        "                        new_bn.bias[new_idx] = layer.bias[old_idx]\n",
        "                        new_bn.running_mean[new_idx] = layer.running_mean[old_idx]\n",
        "                        new_bn.running_var[new_idx] = layer.running_var[old_idx]\n",
        "                new_layers.append(new_bn)\n",
        "            else:\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            # Ajuster la première couche Linear après Flatten\n",
        "            if len([l for l in new_layers if isinstance(l, nn.Linear)]) == 0:\n",
        "                # Calculer la nouvelle taille après les couches conv\n",
        "                new_input_features = calculate_linear_input_size(new_layers, input_size)\n",
        "\n",
        "                new_linear = nn.Linear(\n",
        "                    in_features=new_input_features,\n",
        "                    out_features=layer.out_features,\n",
        "                    bias=(layer.bias is not None)\n",
        "                ).to(device)\n",
        "\n",
        "                # Copier les poids (avec adaptation de taille)\n",
        "                with torch.no_grad():\n",
        "                    min_features = min(new_input_features, layer.in_features)\n",
        "                    new_linear.weight[:, :min_features] = layer.weight[:, :min_features]\n",
        "                    if layer.bias is not None:\n",
        "                        new_linear.bias[:] = layer.bias[:]\n",
        "\n",
        "                new_layers.append(new_linear)\n",
        "            else:\n",
        "                # Autres couches Linear: copier tel quel\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "        else:\n",
        "            # Autres couches (ReLU, MaxPool, Dropout, Flatten)\n",
        "            new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "    return nn.Sequential(*new_layers).to(device)\n",
        "\n",
        "def calculate_linear_input_size(conv_layers, input_size):\n",
        "    \"\"\"\n",
        "    Calcule la taille d'entrée pour la première couche Linear\n",
        "    en simulant une passe avant à travers les couches conv.\n",
        "    \"\"\"\n",
        "    channels, h, w = input_size\n",
        "\n",
        "    for layer in conv_layers:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Calculer nouvelle taille après convolution\n",
        "            kernel_size = layer.kernel_size[0] if isinstance(layer.kernel_size, tuple) else layer.kernel_size\n",
        "            stride = layer.stride[0] if isinstance(layer.stride, tuple) else layer.stride\n",
        "            padding = layer.padding[0] if isinstance(layer.padding, tuple) else layer.padding\n",
        "\n",
        "            h = (h + 2*padding - kernel_size) // stride + 1\n",
        "            w = (w + 2*padding - kernel_size) // stride + 1\n",
        "            channels = layer.out_channels\n",
        "\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            kernel_size = layer.kernel_size\n",
        "            stride = layer.stride if layer.stride else kernel_size\n",
        "            h = h // stride\n",
        "            w = w // stride\n",
        "\n",
        "    return channels * h * w\n",
        "\n",
        "def structured_channel_pruning(model, importance_dict, keep_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Pruning structuré: supprime des canaux entiers basé sur leur importance moyenne.\n",
        "    Plus efficace pour réduire réellement les paramètres.\n",
        "\n",
        "    Args:\n",
        "        model: modèle à pruner\n",
        "        importance_dict: dict[layer][out_ch][in_ch] = importance\n",
        "        keep_ratio: fraction de canaux à conserver (0.5 = 50%)\n",
        "    \"\"\"\n",
        "    # Calculer l'importance moyenne par canal de sortie\n",
        "    channel_importance = {}\n",
        "\n",
        "    for layer_idx in importance_dict:\n",
        "        channel_importance[layer_idx] = {}\n",
        "        for out_ch in importance_dict[layer_idx]:\n",
        "            # Moyenne d'importance sur tous les canaux d'entrée pour ce canal de sortie\n",
        "            avg_importance = np.mean(list(importance_dict[layer_idx][out_ch].values()))\n",
        "            channel_importance[layer_idx][out_ch] = avg_importance\n",
        "\n",
        "    # Sélectionner les top canaux pour chaque couche\n",
        "    top_indices = []\n",
        "    for layer_idx in channel_importance:\n",
        "        # Trier par importance décroissante\n",
        "        channels_by_importance = sorted(\n",
        "            channel_importance[layer_idx].items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=False\n",
        "        )\n",
        "\n",
        "        # Garder le top keep_ratio\n",
        "        n_keep = max(1, int(len(channels_by_importance) * keep_ratio))\n",
        "        top_channels = [ch for ch, _ in channels_by_importance[:n_keep]]\n",
        "\n",
        "        # Créer les indices pour tous les canaux d'entrée de ces canaux de sortie\n",
        "        for out_ch in top_channels:\n",
        "            for in_ch in importance_dict[layer_idx][out_ch]:\n",
        "                top_indices.append((layer_idx, out_ch, in_ch))\n",
        "\n",
        "    return top_indices\n",
        "\n",
        "def test_effective_pruning(model, importance_dict, trn_dl, val_dl, loss_fn, keep_ratios=[0.8]):\n",
        "    \"\"\"\n",
        "    Test de pruning avec réduction EFFECTIVE des paramètres.\n",
        "    \"\"\"\n",
        "    print(\"Test de Pruning avec Réduction Effective des Paramètres\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Baseline\n",
        "    orig_params = sum(p.numel() for p in model.parameters())\n",
        "    orig_loss, orig_acc = evaluate_model(model, val_dl, loss_fn)\n",
        "\n",
        "    print(f\"Modèle original: {orig_acc:.4f} accuracy, {orig_params:,} paramètres\")\n",
        "    print()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for keep_ratio in keep_ratios:\n",
        "        print(f\"Test avec keep_ratio = {keep_ratio} ({keep_ratio*100:.0f}% des canaux)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Pruning structuré par canaux\n",
        "            top_indices = structured_channel_pruning(model, importance_dict, keep_ratio)\n",
        "            print(f\"Filtres sélectionnés: {len(top_indices)}\")\n",
        "\n",
        "            # Appliquer le pruning EFFECTIF\n",
        "            pruned_model = prune_model_by_filters_removal(model, top_indices, input_size=(3, 32, 32))\n",
        "\n",
        "            # Vérifier la réduction\n",
        "            pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
        "            actual_reduction = (orig_params - pruned_params) / orig_params * 100\n",
        "\n",
        "            print(f\"Paramètres avant: {orig_params:,}\")\n",
        "            print(f\"Paramètres après: {pruned_params:,}\")\n",
        "            print(f\"Réduction RÉELLE: {actual_reduction:.1f}%\")\n",
        "\n",
        "            # Test du modèle pruné\n",
        "            val_loss_before, val_acc_before = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "            print(f\"Accuracy avant fine-tuning: {val_acc_before:.4f}\")\n",
        "\n",
        "            # Fine-tuning\n",
        "            print(\"Fine-tuning...\")\n",
        "            pruned_model = fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=5, lr=1e-3)\n",
        "\n",
        "            val_loss_final, val_acc_final = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "\n",
        "            accuracy_retention = (val_acc_final / orig_acc) * 100\n",
        "            compression_ratio = orig_params / pruned_params\n",
        "\n",
        "            print(f\"Accuracy finale: {val_acc_final:.4f}\")\n",
        "            print(f\"Rétention accuracy: {accuracy_retention:.1f}%\")\n",
        "            print(f\"Ratio de compression: {compression_ratio:.1f}x\")\n",
        "            print()\n",
        "\n",
        "            results.append({\n",
        "                'keep_ratio': keep_ratio,\n",
        "                'actual_reduction': actual_reduction,\n",
        "                'final_accuracy': val_acc_final,\n",
        "                'accuracy_retention': accuracy_retention,\n",
        "                'compression_ratio': compression_ratio,\n",
        "                'pruned_params': pruned_params\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec keep_ratio {keep_ratio}: {e}\")\n",
        "            print()\n",
        "\n",
        "    # Résumé des résultats\n",
        "    print(\"RÉSUMÉ DES RÉSULTATS\")\n",
        "    print(\"=\"*40)\n",
        "    for r in results:\n",
        "        print(f\"Keep {r['keep_ratio']*100:.0f}%: {r['actual_reduction']:.1f}% réduction, \"\n",
        "              f\"{r['accuracy_retention']:.1f}% accuracy, {r['compression_ratio']:.1f}x compression\")\n",
        "\n",
        "    return pruned_model\n",
        "\n",
        "# Fonction d'évaluation (si pas déjà définie)\n",
        "def evaluate_model(model, val_dl, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            device = next(model.parameters()).device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            total_loss += loss_fn(pred, y).item() * x.size(0)\n",
        "            total_correct += (pred.argmax(1) == y).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "def fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=10, lr=1e-4):\n",
        "    \"\"\"Fine-tune le modèle pruné pour récupérer l'accuracy.\"\"\"\n",
        "    from torch.optim import Adam\n",
        "\n",
        "    device = next(pruned_model.parameters()).device\n",
        "    optimizer = Adam(pruned_model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pruned_model.train()\n",
        "        for x, y in trn_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = pruned_model(x)\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation check\n",
        "        val_loss, val_acc = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "        if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    return pruned_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyvjgo6gajdQ",
        "outputId": "798fec4d-c6ef-47af-a292-f6bb73c17803"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "model_test = copy.deepcopy(model) \n",
        "pruned_model = test_effective_pruning(model_test,filter_ranking,train_loader, val_loader, loss_fn) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
