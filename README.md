# D√©ploiement d‚Äôun mod√®le DNN pr√©entra√Æn√© sur le dataset CIFAR-10 pour microcontr√¥leur

Ce projet √©tudie le **d√©ploiement d‚Äôun mod√®le de classification** sur une cible embarqu√©e **STM32**.  
Il est divis√© en deux parties principales :

---

## üß† Partie 1 ‚Äî Conception du mod√®le

Cette premi√®re partie concerne la **conception et l‚Äôoptimisation** du mod√®le via :
- l‚Äô√©laboration d‚Äôune **m√©thode de pruning**,  
- la **s√©lection d‚Äôune architecture personnalis√©e**,  
- et l‚Äô**√©valuation** de cette derni√®re sur le dataset **CIFAR-10**.

### Contenu :
- **`Pruning.pdf`** ‚Äî rapport d√©taillant la m√©thode de pruning et les choix d‚Äôarchitecture du nouveau mod√®le ;  (dossier *Pruning*) 
- **`pruning_modele.ipynb`** ‚Äî d√©monstration pratique de la m√©thode de pruning appliqu√©e au mod√®le. (dossier *Pruning*) 
- **`entrainement_evaluation.ipynb`** - code d'entrainement et d'√©valuation du mod√®le. 
- **`outils.ipynb`** - Ensemble de fonctions, pour la plupart absentes des autres notebooks, mais qui nous ont permis d‚Äôexplorer la m√©thode de **pruning** et son √©valuation.
- **`resultats_pruning.xlxs`** Tableau r√©capitulatif de l'ensemble des r√©sultats obtenus pour les diff√©rentes m√©thodes de pruning (dossier *Pruning*) 
- **`resultats_pruning_bruts.txt`** Document enregistrants l'ensemble des r√©sultats brutes pour les diff√©rentes m√©thodes de pruning (dossier *Pruning*) 
---

## ‚öôÔ∏è Partie 2 ‚Äî D√©ploiement sur microcontr√¥leur

La seconde partie porte sur le **d√©ploiement du mod√®le con√ßu** sur la cible **STM32**.

### Contenu :
- **Fichiers de d√©ploiement** pour le microcontr√¥leur ;  
- **Mod√®le converti** et pr√™t √† √™tre int√©gr√© sur la plateforme embarqu√©e.

---

## üß© Objectif global

L‚Äôobjectif du projet est de **proposer une m√©thodologie compl√®te** allant de la conception d‚Äôun mod√®le de classification optimis√© √† son **d√©ploiement sur une cible embarqu√©e √† ressources limit√©es**.

---
---
# VGG11_CIFAR10_simple - Analyse Architecturale Compl√®te

## üìã Vue d'ensemble

Cette analyse pr√©sente une architecture VGG-11 modifi√©e et optimis√©e pour la classification d'images CIFAR-10. Le mod√®le int√®gre des techniques modernes de r√©gularisation tout en conservant la philosophie architecturale VGG classique. L'architecture VGG, introduite par Simonyan et Zisserman en 2014, a d√©montr√© l'efficacit√© des r√©seaux profonds utilisant exclusivement des filtres de petite taille (3√ó3), principe qui est conserv√© dans cette adaptation.

### Caract√©ristiques principales

- **8 couches convolutionnelles** organis√©es en 6 blocs distincts qui extraient progressivement des features de plus en plus abstraites
- **Classificateur dense** √† 3 couches qui transforme les features extraites en pr√©dictions de classes
- **~1.34 millions de param√®tres** (optimis√© pour CIFAR-10, bien inf√©rieur aux 132M du VGG-11 original)
- **R√©gularisation moderne** : BatchNormalization pour la stabilit√© et SpatialDropout2D pour la robustesse

---

## üèóÔ∏è Architecture

### Philosophie de conception

L'architecture s'inspire du paradigme VGG classique avec des am√©liorations modernes adapt√©es aux d√©fis sp√©cifiques du dataset CIFAR-10 :

- **Filtres uniformes** : Noyaux 3√ó3 exclusivement, permettant un empilement profond avec un nombre de param√®tres r√©duit
- **Profondeur contr√¥l√©e** : 8 couches convolutionnelles, un compromis entre la capacit√© d'apprentissage et le risque de sur-apprentissage
- **R√©gularisation** : BatchNorm apr√®s chaque activation et SpatialDropout2D dans les blocs strat√©giques
- **Adaptation** : Optimis√©e pour images 32√ó32 pixels avec un nombre de filtres maximal de 128 (vs 512 dans VGG original)

### Structure hi√©rarchique

Le mod√®le suit une structure pyramidale √† 4 niveaux o√π les dimensions spatiales diminuent progressivement tandis que la profondeur des canaux augmente. Cette approche classique permet de capturer d'abord des d√©tails fins avec une haute r√©solution, puis des concepts de plus en plus abstraits :

1. **Bloc 1** : Extraction des caract√©ristiques de bas niveau (32 filtres) - d√©tection des contours, transitions de couleurs
2. **Bloc 2** : Consolidation des motifs √©l√©mentaires (32 filtres) avec premi√®re r√©duction spatiale
3. **Bloc 3** : Capture des motifs interm√©diaires (64 filtres) - textures, formes g√©om√©triques simples
4. **Bloc 4** : Approfondissement des features interm√©diaires (64 filtres) avec deuxi√®me r√©duction spatiale
5. **Bloc 5** : Extraction des caract√©ristiques de haut niveau (128 filtres) - parties d'objets, motifs complexes
6. **Bloc 6** : Raffinement final (128 filtres) avec r√©solution spatiale minimale

---

## üìä D√©tails de l'architecture

### Progression des caract√©ristiques

Le tableau ci-dessous montre l'√©volution des dimensions et de la complexit√© √† travers le r√©seau. On observe que le volume d'information cro√Æt initialement, puis d√©cro√Æt progressivement gr√¢ce aux op√©rations de max pooling :

| Bloc | Canaux entr√©e | Canaux sortie | Dimension spatiale | Volume total |
|------|---------------|---------------|--------------------|--------------| 
| 1 | 3 | 32 | 32√ó32 | 32,768 valeurs |
| 2 | 32 | 32 | 32√ó32 ‚Üí 16√ó16 | 8,192 valeurs |
| 3 | 32 | 64 | 16√ó16 | 16,384 valeurs |
| 4 | 64 | 64 | 16√ó16 ‚Üí 8√ó8 | 4,096 valeurs |
| 5 | 64 | 128 | 8√ó8 ‚Üí 4√ó4 | 2,048 valeurs |
| 6 | 128 | 128 | 4√ó4 ‚Üí 2√ó2 | 512 valeurs |

La progression montre une compensation intelligente : quand la r√©solution spatiale diminue de moiti√©, le nombre de canaux double, maintenant ainsi la capacit√© repr√©sentationnelle du r√©seau.

### Classificateur dense

Le classificateur adopte une architecture en pyramide invers√©e, contrastant avec la structure en entonnoir de la partie convolutionnelle. Cette expansion puis contraction permet de combiner richement les features extraites :

- **Flatten** : 512 dimensions (128 √ó 2 √ó 2), transformation du tenseur 3D en vecteur 1D
- **Dense 1** : 512 ‚Üí 1024 (ReLU + Dropout 0.3) - expansion pour cr√©er un espace de combinaisons riche
- **Dense 2** : 1024 ‚Üí 512 (ReLU + Dropout 0.3) - synth√®se et compression de l'information discriminante
- **Sortie** : 512 ‚Üí 10 (Softmax) - projection vers les 10 classes avec distribution de probabilit√©

---

## üîß Innovations architecturales

### BatchNormalization strat√©gique

Int√©gr√©e apr√®s chaque activation ReLU dans tous les blocs convolutionnels, la BatchNormalization normalise la distribution des activations en ajustant leur moyenne √† 0 et leur variance √† 1. Cette technique apporte plusieurs b√©n√©fices mesurables :

- **Stabiliser** la variance interne des activations, r√©duisant le ph√©nom√®ne d'Internal Covariate Shift
- **Acc√©l√©rer** la convergence durant l'entra√Ænement en permettant des learning rates plus √©lev√©s
- **R√©gulariser** implicitement le mod√®le gr√¢ce au bruit introduit par les statistiques de batch
- **Renforcer** la robustesse √† l'initialisation des poids, facilitant l'exp√©rimentation

La formule math√©matique appliqu√©e est : `xÃÇ = (x - Œº_batch) / ‚àö(œÉ¬≤_batch + Œµ)` suivie de `y = Œ≥ * xÃÇ + Œ≤`, o√π Œ≥ et Œ≤ sont des param√®tres apprenables qui permettent au r√©seau de retrouver la distribution originale si n√©cessaire.

### SpatialDropout2D

Am√©lioration significative par rapport au dropout classique, sp√©cifiquement con√ßue pour les architectures convolutionnelles. Le SpatialDropout2D d√©sactive al√©atoirement des feature maps enti√®res plut√¥t que des neurones individuels :

| Caract√©ristique | Dropout Classique | SpatialDropout2D |
|-----------------|-------------------|------------------|
| Unit√© de suppression | Neurones individuels | **Canaux complets** (25% avec taux 0.25) |
| Pr√©servation spatiale | ‚ùå Non | ‚úÖ Oui - maintient les corr√©lations spatiales |
| Corr√©lations locales | Perturb√©es par le bruit | Maintenues dans chaque feature map |
| Efficacit√© convolutionnelle | Limit√©e | Optimale pour les CNN |

Cette approche est plus efficace car les valeurs au sein d'une m√™me feature map sont fortement corr√©l√©es spatialement (elles proviennent du m√™me filtre). D√©sactiver des pixels al√©atoires ne forcerait pas le r√©seau √† d√©velopper des repr√©sentations robustes, alors que supprimer des canaux entiers oblige le r√©seau √† ne pas d√©pendre excessivement de certaines features sp√©cifiques.

### S√©quence des op√©rations

**S√©quence appliqu√©e** : Conv2D ‚Üí ReLU ‚Üí [SpatialDropout2D] ‚Üí BatchNorm ‚Üí [MaxPool2D]

Cette s√©quence pr√©sente l'avantage de normaliser les activations apr√®s application du dropout, stabilisant ainsi la distribution des donn√©es d'entr√©e de la couche suivante. L'activation ReLU est appliqu√©e avant la normalisation, ce qui permet de normaliser une distribution d√©j√† filtr√©e par la non-lin√©arit√©.

---

## üìà Distribution des param√®tres

### R√©partition par composant

Le tableau d√©taill√© ci-dessous r√©v√®le la distribution compl√®te des param√®tres √† travers l'architecture. On observe un d√©s√©quilibre notable vers les couches denses qui concentrent la majorit√© des param√®tres :

| Composant | Param√®tres | Pourcentage | Calcul d√©taill√© |
|-----------|------------|-------------|-----------------|
| **Couches Convolutionnelles** | **287,008** | **21.4%** | |
| Conv2D (3‚Üí32) | 896 | 0.07% | 3√ó3√ó3√ó32 + 32 biais |
| Conv2D (32‚Üí32) | 9,248 | 0.69% | 3√ó3√ó32√ó32 + 32 biais |
| Conv2D (32‚Üí64) | 18,496 | 1.38% | 3√ó3√ó32√ó64 + 64 biais |
| Conv2D (64‚Üí64) | 36,928 | 2.75% | 3√ó3√ó64√ó64 + 64 biais |
| Conv2D (64‚Üí128) | 73,856 | 5.50% | 3√ó3√ó64√ó128 + 128 biais |
| Conv2D (128‚Üí128) | 147,584 | 11.0% | 3√ó3√ó128√ó128 + 128 biais |
| **BatchNormalization** | **896** | **0.07%** | |
| BN (32 canaux) √ó 2 | 128 | 0.01% | (Œ≥ + Œ≤) √ó 32 √ó 2 blocs |
| BN (64 canaux) √ó 2 | 256 | 0.02% | (Œ≥ + Œ≤) √ó 64 √ó 2 blocs |
| BN (128 canaux) √ó 2 | 512 | 0.04% | (Œ≥ + Œ≤) √ó 128 √ó 2 blocs |
| **Couches Denses** | **1,055,242** | **78.6%** | |
| Dense (512‚Üí1024) | 525,312 | 39.1% | 512√ó1024 + 1024 biais |
| Dense (1024‚Üí512) | 524,800 | 39.1% | 1024√ó512 + 512 biais |
| Dense (512‚Üí10) | 5,130 | 0.38% | 512√ó10 + 10 biais |
| **TOTAL** | **~1,343,146** | **100%** | |

### Analyse de l'efficacit√©

**Observations critiques** r√©v√©lant les forces et faiblesses de l'architecture :

- **78.6%** des param√®tres concentr√©s dans seulement 3 couches denses du classificateur
- Les deux premi√®res couches denses contiennent √† elles seules plus de 1 million de param√®tres
- Cette concentration peut cr√©er un risque de sur-apprentissage dans le classificateur
- Les couches convolutionnelles ne repr√©sentent que **21.4%** des param√®tres mais effectuent l'essentiel du travail d'extraction de features
- La BatchNormalization ajoute un overhead param√©trique n√©gligeable (0.07%) pour un b√©n√©fice substantiel
- Meilleur √©quilibre que certaines architectures CNN basiques o√π les couches denses peuvent repr√©senter >90% des param√®tres

---

## üÜö Comparaison avec VGG-11 Original

### Adaptations pour CIFAR-10

Le tableau comparatif suivant illustre les modifications majeures apport√©es pour adapter VGG-11 au contexte sp√©cifique de CIFAR-10 :

| Aspect | VGG-11 Original | Version CIFAR-10 | Justification |
|--------|-----------------|------------------|---------------|
| R√©solution d'entr√©e | 224√ó224 | 32√ó32 | CIFAR-10 natif |
| Nombre de couches conv | 8 | 8 | Maintenu identique |
| Filtres maximum | 512 | 128 | R√©duit de 75% |
| Premi√®re couche dense | 25,088 entr√©es | 512 entr√©es | Adapt√© √† la r√©solution |
| Taille classificateur | 25K‚Üí4K‚Üí4K‚Üí1K | 512‚Üí1K‚Üí512‚Üí10 | Simplifi√© pour 10 classes |
| BatchNormalization | ‚ùå Non | ‚úÖ Oui | Technique moderne |
| SpatialDropout | ‚ùå Non | ‚úÖ Oui | R√©gularisation spatiale |
| Param√®tres totaux | 132M | 1.34M | R√©duction de 99% |

### Justifications des modifications

**R√©duction de la complexit√© des filtres** - La limitation √† 128 filtres maximum (vs 512 dans VGG original) s'explique par plusieurs facteurs convergents :

- **R√©solution d'entr√©e r√©duite** : Avec des images 32√ó32 au lieu de 224√ó224, il y a simplement moins d'information spatiale √† encoder
- **Complexit√© du dataset** : CIFAR-10 avec 10 classes est consid√©rablement plus simple qu'ImageNet avec 1000 classes
- **Pr√©vention du sur-apprentissage** : Une capacit√© excessive conduirait √† m√©moriser les donn√©es d'entra√Ænement plut√¥t qu'√† g√©n√©raliser

**Int√©gration de la r√©gularisation moderne** - L'ajout de BatchNorm et SpatialDropout r√©pond aux standards actuels d'entra√Ænement :

- **Stabilit√© d'entra√Ænement** : BatchNorm r√©duit drastiquement la sensibilit√© √† l'initialisation des poids
- **G√©n√©ralisation** : SpatialDropout pr√©vient le sur-apprentissage en for√ßant la redondance des features
- **Performance** : Ces techniques permettent d'atteindre la convergence plus rapidement avec moins d'√©poques

---

## üìä Performances attendues

### Estimation th√©orique

Bas√© sur l'architecture et les techniques employ√©es, ainsi que sur les r√©sultats typiques d'architectures similaires sur CIFAR-10 :

| M√©trique | Estimation | Explication |
|----------|------------|-------------|
| Accuracy CIFAR-10 | 88-93% | Architecture VGG + r√©gularisation moderne |
| √âpoques de convergence | 50-80 | BatchNorm acc√©l√®re mais dataset complexe |
| Temps d'entra√Ænement | Mod√©r√© | ~5-10 min/epoch sur GPU moderne |
| Stabilit√© | √âlev√©e | BatchNorm assure une convergence stable |
| G√©n√©ralisation | Bonne | R√©gularisation triple (Spatial+Batch+Drop) |
| Robustesse | √âlev√©e | Architecture VGG √©prouv√©e depuis 2014 |

### Benchmarking architectural

Positionnement du mod√®le par rapport aux architectures de r√©f√©rence sur CIFAR-10. Ce tableau permet de situer les performances attendues dans le contexte plus large de l'√©tat de l'art :

| Architecture | Param√®tres | CIFAR-10 Acc. | Complexit√© | Commentaire |
|--------------|------------|---------------|------------|-------------|
| LeNet-5 | 60K | ~70% | Faible | Trop simple pour CIFAR-10 |
| AlexNet Adapt√© | 2M | ~85% | Moyenne | Performance correcte mais dat√©e |
| **VGG-11 CIFAR-10** | **1.34M** | **~90%** | **Moyenne** | **Bon compromis** |
| ResNet-20 | 270K | ~92% | Moyenne | Skip connections efficaces |
| DenseNet-40 | 1M | ~94% | √âlev√©e | Dense connections avanc√©es |
| EfficientNet-B0 | 5M | ~95% | √âlev√©e | Architecture state-of-the-art |

Le mod√®le se positionne comme une baseline solide : pas le plus performant, mais un excellent √©quilibre entre simplicit√©, performances et ressources n√©cessaires.

---

## ‚ö†Ô∏è Limitations et d√©fis

### Limitations architecturales

Malgr√© ses qualit√©s, l'architecture pr√©sente plusieurs limitations inh√©rentes √† sa conception s√©quentielle pure :

- **Absence de skip connections** : Contrairement aux architectures ResNet qui utilisent des connexions r√©siduelles, ce mod√®le peut souffrir de probl√®mes de gradient dans les couches profondes
- **Pooling agressif** : Quatre op√©rations de max pooling r√©duisent l'image de 32√ó32 √† 2√ó2, entra√Ænant une perte potentielle d'information spatiale fine qui pourrait √™tre discriminante
- **Architecture s√©quentielle** : Pas de parall√©lisation des branches comme dans Inception, limitant la diversit√© des features √† chaque niveau
- **Classificateur dense dominant** : 78.6% des param√®tres concentr√©s dans 3 couches denses peut cr√©er un goulot d'√©tranglement et un risque de sur-apprentissage localis√©

### D√©fis computationnels

Plusieurs aspects de l'architecture posent des d√©fis pratiques lors de l'entra√Ænement et du d√©ploiement :

- **M√©moire** : Les feature maps volumineuses des premi√®res couches (32√ó32√ó32) n√©cessitent une m√©moire GPU substantielle, surtout avec des batchs de grande taille
- **BatchNorm** : D√©pendance √† la taille du batch pour des statistiques fiables ; performance peut se d√©grader avec des batchs tr√®s petits (<16)
- **R√©gularisation** : √âquilibrage d√©licat entre dropout et BatchNorm n√©cessaire ; trop de r√©gularisation peut sous-fitter, pas assez peut sur-fitter
- **Temps d'inf√©rence** : Les couches denses massives ralentissent l'inf√©rence compar√© √† des architectures plus modernes avec Global Average Pooling

---

## üéì Cas d'usage recommand√©s

### ‚úÖ Id√©al pour

Cette architecture brille dans plusieurs contextes sp√©cifiques o√π ses caract√©ristiques sont particuli√®rement adapt√©es :

- **Prototypage rapide** : Architecture simple √† impl√©menter, comprendre et d√©boguer ; excellente pour tester rapidement des id√©es
- **Enseignement** : Illustre parfaitement les concepts fondamentaux de CNN (convolution, pooling, normalisation, dropout)
- **Baseline** : Point de comparaison solide et reproductible pour exp√©rimenter d'autres architectures ou techniques
- **Datasets similaires** : Images de faible/moyenne r√©solution (32√ó32 √† 64√ó64) avec un nombre mod√©r√© de classes (10-100)
- **Ressources limit√©es** : 1.34M param√®tres permet l'entra√Ænement sur GPU grand public (GTX 1660, RTX 3060) avec batchs raisonnables

### ‚ùå Moins adapt√© pour

Certains contextes n√©cessitent des architectures plus sp√©cialis√©es o√π ce mod√®le serait sous-optimal :

- **R√©solution √©lev√©e** : Images >128√ó128 n√©cessiteraient plus de couches et de filtres, augmentant drastiquement les param√®tres
- **Tr√®s nombreuses classes** : Au-del√† de 100 classes, le classificateur dense deviendrait d√©mesur√©ment lourd
- **D√©tection fine** : La perte d'information spatiale via 4 poolings rend difficile la localisation pr√©cise d'objets
- **Production critique** : Architectures state-of-the-art (EfficientNet, Vision Transformer) offrent de meilleures performances
- **Inf√©rence temps r√©el** : MobileNet ou SqueezeNet sont significativement plus rapides avec moins de param√®tres

---

## üìö Conclusion

### Synth√®se des forces

Ce mod√®le repr√©sente une **adaptation moderne et r√©ussie** du paradigme VGG pour CIFAR-10, avec plusieurs points forts identifi√©s :

‚úÖ **Architecture √©prouv√©e et stable** - Bas√©e sur VGG, une architecture qui a fait ses preuves depuis 2014  
‚úÖ **R√©gularisation multi-niveaux efficace** - Combinaison de SpatialDropout2D, BatchNorm et Dropout classique  
‚úÖ **Complexit√© param√©trique raisonnable** - 1.34M param√®tres offre un bon compromis capacit√©/g√©n√©ralisation  
‚úÖ **Potentiel de performance √©lev√©** - Estimation de ~90% d'accuracy sur CIFAR-10  
‚úÖ **Excellent pour l'apprentissage** - Code simple et maintenable, concepts clairement illustr√©s  

### Axes de contexte

L'architecture se positionne comme un **excellent compromis entre simplicit√© conceptuelle et performance pratique**. Elle est particuli√®rement adapt√©e pour l'enseignement des concepts de deep learning et le prototypage rapide, tout en maintenant un potentiel de performance suffisant pour des applications r√©elles non critiques.

Le d√©s√©quilibre param√©trique vers le classificateur dense (78.6%) sugg√®re une opportunit√© d'optimisation via des techniques comme le Global Average Pooling, qui pourrait r√©duire drastiquement le nombre de param√®tres tout en maintenant voire am√©liorant les performances.

Cette architecture constitue une **baseline solide et reproductible** pour exp√©rimenter avec diff√©rentes techniques de r√©gularisation, strat√©gies d'entra√Ænement, et modifications architecturales sur le dataset CIFAR-10.



*Document g√©n√©r√© pour analyse architecturale d√©taill√©e - VGG11_CIFAR10_simple*
