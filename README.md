# D√©ploiement d‚Äôun mod√®le DNN pr√©entra√Æn√© sur le dataset CIFAR-10 pour microcontr√¥leur

Ce projet √©tudie le **d√©ploiement d‚Äôun mod√®le de classification** sur une cible embarqu√©e **STM32**.  

---

# üß© Sommaire 

## üß† Partie 1 ‚Äî Conception du mod√®le

Cette premi√®re partie concerne la **conception et l‚Äôoptimisation** du mod√®le via :
- l‚Äô√©laboration d‚Äôune **m√©thode de pruning**,  
- la **s√©lection d‚Äôune architecture personnalis√©e**,  
- et l‚Äô**√©valuation** de cette derni√®re sur le dataset **CIFAR-10**.

### Contenu :
- **`Pruning.pdf`** ‚Äî rapport d√©taillant la m√©thode de pruning et les choix d‚Äôarchitecture du nouveau mod√®le ;  (dossier *Pruning*) 
- **`pruning_modele.ipynb`** ‚Äî d√©monstration pratique de la m√©thode de pruning appliqu√©e au mod√®le. (dossier *Pruning*) 
- **`entrainement_evaluation.ipynb`** - code d'entrainement et d'√©valuation du mod√®le. 
- **`outils.ipynb`** - Ensemble de fonctions, pour la plupart absentes des autres notebooks, mais qui nous ont permis d‚Äôexplorer la m√©thode de **pruning** et son √©valuation.
- **`resultats_pruning.xlxs`** Tableau r√©capitulatif de l'ensemble des r√©sultats obtenus pour les diff√©rentes m√©thodes de pruning (dossier *Pruning*) 
- **`resultats_pruning_bruts.txt`** Document enregistrants l'ensemble des r√©sultats brutes pour les diff√©rentes m√©thodes de pruning (dossier *Pruning*) 
---

## ‚öôÔ∏è Partie 2 ‚Äî D√©ploiement sur microcontr√¥leur

La seconde partie porte sur le **d√©ploiement du mod√®le con√ßu** sur la cible **STM32**.

### Contenu :
- **Fichiers de d√©ploiement** pour le microcontr√¥leur ;  
- **Mod√®le converti** et pr√™t √† √™tre int√©gr√© sur la plateforme embarqu√©e.

---
## üí£ Partie 3 - Attaque du mod√®le 

Cette troisi√®me partie porte sur l'attaque du mod√®le d√©ploy√© sur le microcontr√¥leur. 
### Contenu : 
- xxxxx
- xxxxx

# üìñ Documentation

## Analyse du mod√®le existant


Cette analyse pr√©sente une architecture VGG-11 modifi√©e et optimis√©e pour la classification d'images CIFAR-10. Le mod√®le int√®gre des techniques modernes de r√©gularisation tout en conservant la philosophie architecturale VGG classique. L'architecture VGG, introduite par Simonyan et Zisserman en 2014, a d√©montr√© l'efficacit√© des r√©seaux profonds utilisant exclusivement des filtres de petite taille (3√ó3), principe qui est conserv√© dans cette adaptation.

### Caract√©ristiques principales

- **8 couches convolutionnelles** organis√©es en 6 blocs distincts qui extraient progressivement des features de plus en plus abstraites
- **Classificateur dense** √† 3 couches qui transforme les features extraites en pr√©dictions de classes
- **~1.34 millions de param√®tres** (optimis√© pour CIFAR-10, bien inf√©rieur aux 132M du VGG-11 original)
- **R√©gularisation moderne** : BatchNormalization pour la stabilit√© et SpatialDropout2D pour la robustesse



## üèóÔ∏è Architecture

### Philosophie de conception

L'architecture s'inspire du paradigme VGG classique avec des am√©liorations modernes adapt√©es aux d√©fis sp√©cifiques du dataset CIFAR-10 :

- **Filtres uniformes** : Noyaux 3√ó3 exclusivement, permettant un empilement profond avec un nombre de param√®tres r√©duit
- **Profondeur contr√¥l√©e** : 8 couches convolutionnelles, un compromis entre la capacit√© d'apprentissage et le risque de sur-apprentissage
- **R√©gularisation** : BatchNorm apr√®s chaque activation et SpatialDropout2D dans les blocs strat√©giques
- **Adaptation** : Optimis√©e pour images 32√ó32 pixels avec un nombre de filtres maximal de 128 (vs 512 dans VGG original)

### Structure hi√©rarchique

Le mod√®le suit une structure pyramidale √† 4 niveaux o√π les dimensions spatiales diminuent progressivement tandis que la profondeur des canaux augmente. Cette approche classique permet de capturer d'abord des d√©tails fins avec une haute r√©solution, puis des concepts de plus en plus abstraits :

1. **Bloc 1** : Extraction des caract√©ristiques de bas niveau (32 filtres) - d√©tection des contours, transitions de couleurs
2. **Bloc 2** : Consolidation des motifs √©l√©mentaires (32 filtres) avec premi√®re r√©duction spatiale
3. **Bloc 3** : Capture des motifs interm√©diaires (64 filtres) - textures, formes g√©om√©triques simples
4. **Bloc 4** : Approfondissement des features interm√©diaires (64 filtres) avec deuxi√®me r√©duction spatiale
5. **Bloc 5** : Extraction des caract√©ristiques de haut niveau (128 filtres) - parties d'objets, motifs complexes
6. **Bloc 6** : Raffinement final (128 filtres) avec r√©solution spatiale minimale



## üìä D√©tails de l'architecture

### Progression des caract√©ristiques

Le tableau ci-dessous montre l'√©volution des dimensions et de la complexit√© √† travers le r√©seau. On observe que le volume d'information cro√Æt initialement, puis d√©cro√Æt progressivement gr√¢ce aux op√©rations de max pooling :

| Bloc | Canaux entr√©e | Canaux sortie | Dimension spatiale | Volume total |
|------|---------------|---------------|--------------------|--------------| 
| 1 | 3 | 32 | 32√ó32 | 32,768 valeurs |
| 2 | 32 | 32 | 32√ó32 ‚Üí 16√ó16 | 8,192 valeurs |
| 3 | 32 | 64 | 16√ó16 | 16,384 valeurs |
| 4 | 64 | 64 | 16√ó16 ‚Üí 8√ó8 | 4,096 valeurs |
| 5 | 64 | 128 | 8√ó8 ‚Üí 4√ó4 | 2,048 valeurs |
| 6 | 128 | 128 | 4√ó4 ‚Üí 2√ó2 | 512 valeurs |

La progression montre une compensation intelligente : quand la r√©solution spatiale diminue de moiti√©, le nombre de canaux double, maintenant ainsi la capacit√© repr√©sentationnelle du r√©seau.

### Classificateur dense

Le classificateur adopte une architecture en pyramide invers√©e, contrastant avec la structure en entonnoir de la partie convolutionnelle. Cette expansion puis contraction permet de combiner richement les features extraites :

- **Flatten** : 512 dimensions (128 √ó 2 √ó 2), transformation du tenseur 3D en vecteur 1D
- **Dense 1** : 512 ‚Üí 1024 (ReLU + Dropout 0.3) - expansion pour cr√©er un espace de combinaisons riche
- **Dense 2** : 1024 ‚Üí 512 (ReLU + Dropout 0.3) - synth√®se et compression de l'information discriminante
- **Sortie** : 512 ‚Üí 10 (Softmax) - projection vers les 10 classes avec distribution de probabilit√©



## üîß Innovations architecturales

### BatchNormalization strat√©gique

Int√©gr√©e apr√®s chaque activation ReLU dans tous les blocs convolutionnels, la BatchNormalization normalise la distribution des activations en ajustant leur moyenne √† 0 et leur variance √† 1. Cette technique apporte plusieurs b√©n√©fices mesurables :

- **Stabiliser** la variance interne des activations, r√©duisant le ph√©nom√®ne d'Internal Covariate Shift
- **Acc√©l√©rer** la convergence durant l'entra√Ænement en permettant des learning rates plus √©lev√©s
- **R√©gulariser** implicitement le mod√®le gr√¢ce au bruit introduit par les statistiques de batch
- **Renforcer** la robustesse √† l'initialisation des poids, facilitant l'exp√©rimentation

La formule math√©matique appliqu√©e est : `xÃÇ = (x - Œº_batch) / ‚àö(œÉ¬≤_batch + Œµ)` suivie de `y = Œ≥ * xÃÇ + Œ≤`, o√π Œ≥ et Œ≤ sont des param√®tres apprenables qui permettent au r√©seau de retrouver la distribution originale si n√©cessaire.

### SpatialDropout2D

Am√©lioration significative par rapport au dropout classique, sp√©cifiquement con√ßue pour les architectures convolutionnelles. Le SpatialDropout2D d√©sactive al√©atoirement des feature maps enti√®res plut√¥t que des neurones individuels :

| Caract√©ristique | Dropout Classique | SpatialDropout2D |
|-----------------|-------------------|------------------|
| Unit√© de suppression | Neurones individuels | **Canaux complets** (25% avec taux 0.25) |
| Pr√©servation spatiale | ‚ùå Non | ‚úÖ Oui - maintient les corr√©lations spatiales |
| Corr√©lations locales | Perturb√©es par le bruit | Maintenues dans chaque feature map |
| Efficacit√© convolutionnelle | Limit√©e | Optimale pour les CNN |

Cette approche est plus efficace car les valeurs au sein d'une m√™me feature map sont fortement corr√©l√©es spatialement (elles proviennent du m√™me filtre). D√©sactiver des pixels al√©atoires ne forcerait pas le r√©seau √† d√©velopper des repr√©sentations robustes, alors que supprimer des canaux entiers oblige le r√©seau √† ne pas d√©pendre excessivement de certaines features sp√©cifiques.

### S√©quence des op√©rations

**S√©quence appliqu√©e** : Conv2D ‚Üí ReLU ‚Üí [SpatialDropout2D] ‚Üí BatchNorm ‚Üí [MaxPool2D]

Cette s√©quence pr√©sente l'avantage de normaliser les activations apr√®s application du dropout, stabilisant ainsi la distribution des donn√©es d'entr√©e de la couche suivante. L'activation ReLU est appliqu√©e avant la normalisation, ce qui permet de normaliser une distribution d√©j√† filtr√©e par la non-lin√©arit√©.



## üìà Distribution des param√®tres

### R√©partition par composant

Le tableau d√©taill√© ci-dessous r√©v√®le la distribution compl√®te des param√®tres √† travers l'architecture. On observe un d√©s√©quilibre notable vers les couches denses qui concentrent la majorit√© des param√®tres :

| Composant | Param√®tres | Pourcentage | Calcul d√©taill√© |
|-----------|------------|-------------|-----------------|
| **Couches Convolutionnelles** | **287,008** | **21.4%** | |
| Conv2D (3‚Üí32) | 896 | 0.07% | 3√ó3√ó3√ó32 + 32 biais |
| Conv2D (32‚Üí32) | 9,248 | 0.69% | 3√ó3√ó32√ó32 + 32 biais |
| Conv2D (32‚Üí64) | 18,496 | 1.38% | 3√ó3√ó32√ó64 + 64 biais |
| Conv2D (64‚Üí64) | 36,928 | 2.75% | 3√ó3√ó64√ó64 + 64 biais |
| Conv2D (64‚Üí128) | 73,856 | 5.50% | 3√ó3√ó64√ó128 + 128 biais |
| Conv2D (128‚Üí128) | 147,584 | 11.0% | 3√ó3√ó128√ó128 + 128 biais |
| **BatchNormalization** | **896** | **0.07%** | |
| BN (32 canaux) √ó 2 | 128 | 0.01% | (Œ≥ + Œ≤) √ó 32 √ó 2 blocs |
| BN (64 canaux) √ó 2 | 256 | 0.02% | (Œ≥ + Œ≤) √ó 64 √ó 2 blocs |
| BN (128 canaux) √ó 2 | 512 | 0.04% | (Œ≥ + Œ≤) √ó 128 √ó 2 blocs |
| **Couches Denses** | **1,055,242** | **78.6%** | |
| Dense (512‚Üí1024) | 525,312 | 39.1% | 512√ó1024 + 1024 biais |
| Dense (1024‚Üí512) | 524,800 | 39.1% | 1024√ó512 + 512 biais |
| Dense (512‚Üí10) | 5,130 | 0.38% | 512√ó10 + 10 biais |
| **TOTAL** | **~1,343,146** | **100%** | |

### Analyse de l'efficacit√©

**Observations critiques** r√©v√©lant les forces et faiblesses de l'architecture :

- **78.6%** des param√®tres concentr√©s dans seulement 3 couches denses du classificateur
- Les deux premi√®res couches denses contiennent √† elles seules plus de 1 million de param√®tres
- Cette concentration peut cr√©er un risque de sur-apprentissage dans le classificateur
- Les couches convolutionnelles ne repr√©sentent que **21.4%** des param√®tres mais effectuent l'essentiel du travail d'extraction de features
- La BatchNormalization ajoute un overhead param√©trique n√©gligeable (0.07%) pour un b√©n√©fice substantiel
- Meilleur √©quilibre que certaines architectures CNN basiques o√π les couches denses peuvent repr√©senter >90% des param√®tres



## ‚ö†Ô∏è Limitations et d√©fis

### Limitations architecturales

Malgr√© ses qualit√©s, l'architecture pr√©sente plusieurs limitations inh√©rentes √† sa conception s√©quentielle pure :

- **Absence de skip connections** : Contrairement aux architectures ResNet qui utilisent des connexions r√©siduelles, ce mod√®le peut souffrir de probl√®mes de gradient dans les couches profondes
- **Pooling agressif** : Quatre op√©rations de max pooling r√©duisent l'image de 32√ó32 √† 2√ó2, entra√Ænant une perte potentielle d'information spatiale fine qui pourrait √™tre discriminante
- **Architecture s√©quentielle** : Pas de parall√©lisation des branches comme dans Inception, limitant la diversit√© des features √† chaque niveau
- **Classificateur dense dominant** : 78.6% des param√®tres concentr√©s dans 3 couches denses peut cr√©er un goulot d'√©tranglement et un risque de sur-apprentissage localis√©

### D√©fis computationnels

Plusieurs aspects de l'architecture posent des d√©fis pratiques lors de l'entra√Ænement et du d√©ploiement :

- **M√©moire** : Les feature maps volumineuses des premi√®res couches (32√ó32√ó32) n√©cessitent une m√©moire GPU substantielle, surtout avec des batchs de grande taille
- **BatchNorm** : D√©pendance √† la taille du batch pour des statistiques fiables ; performance peut se d√©grader avec des batchs tr√®s petits (<16)
- **R√©gularisation** : √âquilibrage d√©licat entre dropout et BatchNorm n√©cessaire ; trop de r√©gularisation peut sous-fitter, pas assez peut sur-fitter
- **Temps d'inf√©rence** : Les couches denses massives ralentissent l'inf√©rence compar√© √† des architectures plus modernes avec Global Average Pooling


## üìö Conclusion

### Synth√®se des forces

Ce mod√®le repr√©sente une **adaptation moderne et r√©ussie** du paradigme VGG pour CIFAR-10, avec plusieurs points forts identifi√©s :

‚úÖ **Architecture √©prouv√©e et stable** - Bas√©e sur VGG, une architecture qui a fait ses preuves depuis 2014  
‚úÖ **R√©gularisation multi-niveaux efficace** - Combinaison de SpatialDropout2D, BatchNorm et Dropout classique  
‚úÖ **Complexit√© param√©trique raisonnable** - 1.34M param√®tres offre un bon compromis capacit√©/g√©n√©ralisation  
‚úÖ **Potentiel de performance √©lev√©** - Estimation de ~90% d'accuracy sur CIFAR-10  
‚úÖ **Excellent pour l'apprentissage** - Code simple et maintenable, concepts clairement illustr√©s  

---

## Etude du microcontr√¥leur cible 

---

## Evaluation de l'embarquabilit√© du mod√®le intiial 

---

## Conception du nouveau mod√®le 

Voir partie *Conception du mod√®le* et l'√©tude associ√©. 

---

## Embarquabilit√© du mod√®le finale et √©valuation
