{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWQ46INffP_y",
        "outputId": "3a357f0b-0365-4d26-f153-2513b94e8d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 33, 33]           1,568\n",
            "       BatchNorm2d-2           [-1, 32, 33, 33]              64\n",
            "              ReLU-3           [-1, 32, 33, 33]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "         Dropout2d-5           [-1, 32, 16, 16]               0\n",
            "            Conv2d-6           [-1, 32, 17, 17]             512\n",
            "            Conv2d-7           [-1, 64, 17, 17]           2,048\n",
            "DepthwiseSeparableConv2d-8           [-1, 64, 17, 17]               0\n",
            "       BatchNorm2d-9           [-1, 64, 17, 17]             128\n",
            "             ReLU-10           [-1, 64, 17, 17]               0\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 6, 6]               0\n",
            "        Dropout2d-12             [-1, 64, 6, 6]               0\n",
            "          Flatten-13                 [-1, 2304]               0\n",
            "           Linear-14                  [-1, 128]         295,040\n",
            "             ReLU-15                  [-1, 128]               0\n",
            "          Dropout-16                  [-1, 128]               0\n",
            "           Linear-17                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 300,650\n",
            "Trainable params: 300,650\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.61\n",
            "Params size (MB): 1.15\n",
            "Estimated Total Size (MB): 2.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "def get_optimized_model_v1(device='cpu'):\n",
        "    \"\"\"\n",
        "    Optimisation principale: Réduction dramatique du classificateur\n",
        "    Réduction estimée: ~85% des paramètres\n",
        "    Impact performance: Minimal (< 2% accuracy)\n",
        "    \"\"\"\n",
        "    model = nn.Sequential(\n",
        "        # Bloc 1 - INCHANGÉ (extraction critique)\n",
        "        nn.Conv2d(3, 32, kernel_size=4, padding=2),  # 3 → 32 canaux\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),  # 32x32 → 16x16\n",
        "        nn.Dropout2d(0.25),\n",
        "\n",
        "        # Bloc 2 - INCHANGÉ (patterns importants)\n",
        "        nn.Conv2d(32, 64, kernel_size=4, padding=2),  # 32 → 64 canaux\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),  # 16x16 → 8x8\n",
        "        nn.Dropout2d(0.25),\n",
        "\n",
        "        # Classificateur ULTRA-COMPACT\n",
        "        nn.Flatten(),  # 64 * 8 * 8 = 4096\n",
        "        nn.Linear(4096, 128),  # 512 → 128 (÷4 réduction!)\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),  # Augmentation dropout pour compenser\n",
        "        nn.Linear(128, 10)   # Direct vers sortie\n",
        "    ).to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    return model, loss_fn, optimizer\n",
        "summary(model, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7jsawhzfi7E",
        "outputId": "9f659f12-6b39-4b5b-e542-6486e39dfed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 33, 33]           1,568\n",
            "       BatchNorm2d-2           [-1, 32, 33, 33]              64\n",
            "              ReLU-3           [-1, 32, 33, 33]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "         Dropout2d-5           [-1, 32, 16, 16]               0\n",
            "            Conv2d-6           [-1, 64, 17, 17]          32,832\n",
            "       BatchNorm2d-7           [-1, 64, 17, 17]             128\n",
            "              ReLU-8           [-1, 64, 17, 17]               0\n",
            "         Dropout2d-9           [-1, 64, 17, 17]               0\n",
            "           Conv2d-10           [-1, 10, 17, 17]             650\n",
            "      BatchNorm2d-11           [-1, 10, 17, 17]              20\n",
            "             ReLU-12           [-1, 10, 17, 17]               0\n",
            "AdaptiveAvgPool2d-13             [-1, 10, 1, 1]               0\n",
            "          Flatten-14                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 35,262\n",
            "Trainable params: 35,262\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.55\n",
            "Params size (MB): 0.13\n",
            "Estimated Total Size (MB): 1.70\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def get_optimized_model_v2(device='cpu'):\n",
        "    \"\"\"\n",
        "    Optimisation avancée avec pooling adaptatif\n",
        "    Réduction: ~80% des paramètres\n",
        "    Bénéfice: Meilleure préservation des features spatiales\n",
        "    \"\"\"\n",
        "    model = nn.Sequential(\n",
        "        # Bloc 1 - Feature extraction\n",
        "        nn.Conv2d(3, 32, kernel_size=4, padding=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Dropout2d(0.25),\n",
        "\n",
        "        # Bloc 2 - Pattern recognition\n",
        "        nn.Conv2d(32, 64, kernel_size=4, padding=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # INNOVATION: Adaptive pooling pour taille fixe\n",
        "        nn.AdaptiveAvgPool2d((4, 4)),  # Force 4x4 indépendamment de l'entrée\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Classificateur compact progressif\n",
        "        nn.Flatten(),  # 64 * 4 * 4 = 1024 (÷4 vs original!)\n",
        "        nn.Linear(1024, 256),  # Réduction progressive\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(256, 64),    # Couche intermédiaire\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(64, 10)\n",
        "    ).to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    return model, loss_fn, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XUeV_HCDifYy"
      },
      "outputs": [],
      "source": [
        "class DepthwiseSeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution séparable en profondeur avec kernel_size=4\n",
        "    Réduction massive des paramètres convolutionnels\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, padding=2):\n",
        "        super().__init__()\n",
        "        # Depthwise: chaque canal traité séparément\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels,\n",
        "                                 kernel_size=kernel_size, padding=padding,\n",
        "                                 groups=in_channels, bias=False)\n",
        "        # Pointwise: mélange les canaux avec 1x1\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels,\n",
        "                                 kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "def get_optimized_model_v3(device='cpu'):\n",
        "    \"\"\"\n",
        "    Version MobileNet-inspired avec convolutions séparables\n",
        "    Réduction: ~70% des paramètres conv + classificateur compact\n",
        "    Innovation: Garde l'efficacité du kernel_size=4\n",
        "    \"\"\"\n",
        "    model = nn.Sequential(\n",
        "        # Bloc 1 - Convolution standard pour extraction initiale\n",
        "        nn.Conv2d(3, 32, kernel_size=4, padding=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Dropout2d(0.25),\n",
        "\n",
        "        # Bloc 2 - DEPTHWISE SEPARABLE avec kernel=4\n",
        "        DepthwiseSeparableConv2d(32, 64, kernel_size=4, padding=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.AdaptiveAvgPool2d((6, 6)),  # Pooling adaptatif\n",
        "        nn.Dropout2d(0.3),\n",
        "\n",
        "        # Classificateur ultra-compact\n",
        "        nn.Flatten(),  # 64 * 6 * 6 = 2304\n",
        "        nn.Linear(2304, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),  # Dropout élevé pour compenser la capacité réduite\n",
        "        nn.Linear(128, 10)\n",
        "    ).to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    return model, loss_fn, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJZchUAxjkvw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, targets, device='cpu'):\n",
        "        # Convertir les tableaux NumPy en tenseurs PyTorch\n",
        "        images = torch.from_numpy(images).float() / 255.0\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        # CIFAR10 : images en 3x32x32, format (N, H, W, C) -> (N, C, H, W)\n",
        "        images = images.permute(0, 3, 1, 2)\n",
        "\n",
        "        self.x = images\n",
        "        self.y = targets\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        x = self.x[ix].to(self.device)\n",
        "        y = self.y[ix].to(self.device)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxuWrVfJwmN_",
        "outputId": "1699732f-5027-4161-a97d-c54f074c1535"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:11<00:00, 14.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_folder = '/content/' # This can be any directory you want to download FMNIST to\n",
        "fmnist = datasets.CIFAR10(data_folder, download=True, train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ArwXRVxwrXn",
        "outputId": "60e0cacf-3d6f-4b64-82b7-027a49c1cc0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:03<00:00, 8.10MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 136kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.22MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 8.97MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tr_images = fmnist.data\n",
        "tr_targets = fmnist.targets\n",
        "\n",
        "\n",
        "val_fmnist = datasets.FashionMNIST(data_folder, download=True, train=False)\n",
        "val_images = val_fmnist.data\n",
        "val_targets = val_fmnist.targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ub5mfqr_vTcA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, targets, device='cpu'):\n",
        "        # Convertir les tableaux NumPy en tenseurs PyTorch\n",
        "        images = torch.tensor(images, dtype=torch.float32) / 255.0  # Conversion en tenseur PyTorch\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        # Vérifier la forme des images et les réorganiser si nécessaire\n",
        "        if images.ndimension() == 3:  # Si les images sont de forme (N, H, W) sans canaux\n",
        "            images = images.unsqueeze(3)  # Ajouter un axe de canaux (N, H, W, 1)\n",
        "        elif images.ndimension() == 4 and images.shape[3] != 3:  # Si les images n'ont pas 3 canaux\n",
        "            raise ValueError(f\"Les images doivent avoir 3 canaux, mais ont {images.shape[3]} canaux.\")\n",
        "\n",
        "        # CIFAR10 : images en 3x32x32, format (N, H, W, C) -> (N, C, H, W)\n",
        "        images = images.permute(0, 3, 1, 2)\n",
        "\n",
        "        self.x = images\n",
        "        self.y = targets\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        x = self.x[ix].to(self.device)\n",
        "        y = self.y[ix].to(self.device)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Q0q4UcsvXJ4"
      },
      "outputs": [],
      "source": [
        "def train_batch(x, y, model, opt, loss_fn):\n",
        "    prediction = model(x)\n",
        "    batch_loss = loss_fn(prediction, y)\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return batch_loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(x, y, model):\n",
        "    model.eval()\n",
        "    prediction = model(x)\n",
        "    max_values, argmaxes = prediction.max(-1)\n",
        "    is_correct = argmaxes == y\n",
        "    return is_correct.cpu().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3rFExZdljliY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_data():\n",
        "    train = CIFAR10Dataset(tr_images, tr_targets)\n",
        "    trn_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    val = CIFAR10Dataset(val_images, val_targets)\n",
        "    val_dl = DataLoader(val, batch_size=len(val_images), shuffle=True)\n",
        "    return trn_dl, val_dl\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_loss(x, y, model):\n",
        "    model.eval()\n",
        "    prediction = model(x)\n",
        "    val_loss = loss_fn(prediction, y)\n",
        "    return val_loss.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xPWTlWhvZf4",
        "outputId": "37688986-c2aa-45d0-93a7-1991152b0090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 33, 33]           1,568\n",
            "       BatchNorm2d-2           [-1, 32, 33, 33]              64\n",
            "              ReLU-3           [-1, 32, 33, 33]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "         Dropout2d-5           [-1, 32, 16, 16]               0\n",
            "            Conv2d-6           [-1, 64, 17, 17]          32,832\n",
            "       BatchNorm2d-7           [-1, 64, 17, 17]             128\n",
            "              ReLU-8           [-1, 64, 17, 17]               0\n",
            "         MaxPool2d-9             [-1, 64, 8, 8]               0\n",
            "        Dropout2d-10             [-1, 64, 8, 8]               0\n",
            "          Flatten-11                 [-1, 4096]               0\n",
            "           Linear-12                  [-1, 128]         524,416\n",
            "             ReLU-13                  [-1, 128]               0\n",
            "          Dropout-14                  [-1, 128]               0\n",
            "           Linear-15                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 560,298\n",
            "Trainable params: 560,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.44\n",
            "Params size (MB): 2.14\n",
            "Estimated Total Size (MB): 3.59\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3063548849.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  images = torch.tensor(images, dtype=torch.float32) / 255.0  # Conversion en tenseur PyTorch\n",
            "/tmp/ipython-input-3063548849.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "# Définir le device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Charger les données\n",
        "trn_dl, val_dl = get_data()\n",
        "\n",
        "# Initialiser le modèle\n",
        "model, loss_fn, optimizer = get_optimized_model_v1(device=device)\n",
        "\n",
        "# Afficher un résumé du modèle\n",
        "from torchsummary import summary\n",
        "summary(model, (3, 32, 32))  # CIFAR10 : 3 canaux, 32x32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPcJTGe-x33W",
        "outputId": "caa41979-86bf-43c8-eabb-42b90edad33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "for epoch in range(5):\n",
        "    print(epoch)\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "        train_epoch_losses.append(batch_loss)\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        is_correct = accuracy(x, y, model)\n",
        "        train_epoch_accuracies.extend(is_correct)\n",
        "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
        "\n",
        "    for ix, batch in enumerate(iter(val_dl)):\n",
        "        x, y = batch\n",
        "        val_is_correct = accuracy(x, y, model)\n",
        "        validation_loss = val_loss(x, y, model)\n",
        "    val_epoch_accuracy = np.mean(val_is_correct)\n",
        "\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    train_accuracies.append(train_epoch_accuracy)\n",
        "    val_losses.append(validation_loss)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUgbz6i8x9WP"
      },
      "outputs": [],
      "source": [
        "epochs = np.arange(5)+1\n",
        "import matplotlib.ticker as mtick\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "%matplotlib inline\n",
        "plt.subplot(211)\n",
        "plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
        "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.title('Training and validation loss with CNN')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()\n",
        "plt.subplot(212)\n",
        "plt.plot(epochs, train_accuracies, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')\n",
        "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.title('Training and validation accuracy with CNN')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim(0.8,1)\n",
        "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def get_optimized_vgg5_model(device='cpu'):\n",
        "    \"\"\"\n",
        "    Modèle VGG11 optimisé avec kernels 5x5 et pooling adaptatif\n",
        "    Réduction des paramètres et meilleure préservation des features spatiales\n",
        "    \"\"\"\n",
        "    model = nn.Sequential(\n",
        "        # Bloc 1\n",
        "        nn.Conv2d(3, 32, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=5, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.SpatialDropout2d(0.25) if hasattr(nn, 'SpatialDropout2d') else nn.Dropout2d(0.25),\n",
        "        nn.MaxPool2d(2),\n",
        "\n",
        "        # Bloc 2\n",
        "        nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.SpatialDropout2d(0.25) if hasattr(nn, 'SpatialDropout2d') else nn.Dropout2d(0.25),\n",
        "        nn.MaxPool2d(2),\n",
        "\n",
        "        # Bloc 3\n",
        "        nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.SpatialDropout2d(0.25) if hasattr(nn, 'SpatialDropout2d') else nn.Dropout2d(0.25),\n",
        "        nn.MaxPool2d(2),\n",
        "\n",
        "        # Bloc 4\n",
        "        nn.Conv2d(128, 128, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.SpatialDropout2d(0.25) if hasattr(nn, 'SpatialDropout2d') else nn.Dropout2d(0.25),\n",
        "        nn.MaxPool2d(2),\n",
        "\n",
        "        # INNOVATION: Adaptive pooling pour taille fixe\n",
        "        nn.AdaptiveAvgPool2d((4, 4)),\n",
        "\n",
        "        # Classificateur compact progressif\n",
        "        nn.Flatten(),  # 128 * 4 * 4 = 2048\n",
        "        nn.Linear(2048, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 10)\n",
        "    ).to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    return model, loss_fn, optimizer\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
