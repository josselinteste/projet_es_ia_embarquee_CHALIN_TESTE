{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Data augmentation\n",
        "# -----------------------------\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.RandomErasing(p=0.2)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Importation des donn√©es \n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Mod√®le\n",
        "# -----------------------------\n",
        "def get_model_improved(device='cpu'):\n",
        "    model = nn.Sequential(\n",
        "    # Bloc 1\n",
        "    nn.Conv2d(3, 32, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "\n",
        "    nn.Conv2d(32, 32, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Dropout2d(0.2),\n",
        "\n",
        "    # Bloc 2\n",
        "    nn.Conv2d(32, 64, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "\n",
        "    nn.Conv2d(64, 64, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Dropout2d(0.3),\n",
        "\n",
        "    # Bloc 3\n",
        "    nn.Conv2d(64, 128, kernel_size=4, padding=1, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128),\n",
        "\n",
        "    nn.Conv2d(128, 128, kernel_size=1, padding=0, bias=True),  # bottleneck\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.AdaptiveAvgPool2d((1,1)),\n",
        "\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # Classifier compact\n",
        "    nn.Linear(128, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(128, 10)\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    return model, loss_fn, optimizer, scheduler\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Entra√Ænement + Early stopping + Tracking\n",
        "# -----------------------------\n",
        "def train_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, device='cpu', epochs=30, patience=5):\n",
        "    best_val_loss = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Historique pour les courbes\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = loss_fn(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        train_loss = running_loss / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = loss_fn(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        val_loss /= total\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Enregistrement\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
        "              f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}% | \"\n",
        "              f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss > best_val_loss - 1e-3:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "        else:\n",
        "            patience_counter = 0\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. Affichage des courbes\n",
        "    # -----------------------------\n",
        "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss √©volution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Val Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy ')\n",
        "    plt.title('Accuracy √©volution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Lancement\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, loss_fn, optimizer, scheduler = get_model_improved(device)\n",
        "train_model(model, loss_fn, optimizer, scheduler, train_loader, val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "D√©finition de la m√©thode de pruning structurel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def prune_model_by_filters_removal(model, top_indices_list, input_size=(3, 32, 32)):\n",
        "    \"\"\"\n",
        "    VERSION CORRIG√âE: Supprime r√©ellement les filtres pour r√©duire les param√®tres.\n",
        "\n",
        "    Args:\n",
        "        model: mod√®le PyTorch nn.Sequential\n",
        "        top_indices_list: [(layer_idx, out_ch, in_ch), ...] - filtres √† CONSERVER\n",
        "        input_size: taille d'entr√©e (channels, height, width)\n",
        "\n",
        "    Returns:\n",
        "        Mod√®le prun√© avec moins de param√®tres\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model_copy = copy.deepcopy(model)\n",
        "\n",
        "    # Organiser les filtres √† conserver par couche\n",
        "    filters_to_keep = defaultdict(set)\n",
        "    for layer_idx, out_ch, in_ch in top_indices_list:\n",
        "        filters_to_keep[layer_idx].add(out_ch)\n",
        "\n",
        "    # Convertir en listes tri√©es\n",
        "    for layer_idx in filters_to_keep:\n",
        "        filters_to_keep[layer_idx] = sorted(list(filters_to_keep[layer_idx]))\n",
        "\n",
        "    new_layers = []\n",
        "    conv_layers = [i for i, layer in enumerate(model_copy) if isinstance(layer, nn.Conv2d)]\n",
        "    prev_kept_channels = None\n",
        "\n",
        "    for i, layer in enumerate(model_copy):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Trouver l'index de cette couche conv\n",
        "            conv_idx = conv_layers.index(i)\n",
        "\n",
        "            # D√©terminer les canaux de sortie √† conserver\n",
        "            if conv_idx in filters_to_keep:\n",
        "                keep_out_channels = filters_to_keep[conv_idx]\n",
        "            else:\n",
        "                # Si pas sp√©cifi√©, garder tous les canaux\n",
        "                keep_out_channels = list(range(layer.out_channels))\n",
        "\n",
        "            # D√©terminer les canaux d'entr√©e\n",
        "            if conv_idx == 0:\n",
        "                # Premi√®re couche: garder tous les canaux d'entr√©e (RGB)\n",
        "                new_in_channels = layer.in_channels\n",
        "                keep_in_channels = list(range(layer.in_channels))\n",
        "            else:\n",
        "                # Couches suivantes: s'adapter aux canaux conserv√©s de la couche pr√©c√©dente\n",
        "                new_in_channels = len(prev_kept_channels) if prev_kept_channels else layer.in_channels\n",
        "                keep_in_channels = prev_kept_channels if prev_kept_channels else list(range(layer.in_channels))\n",
        "\n",
        "            new_out_channels = len(keep_out_channels)\n",
        "\n",
        "            # Cr√©er la nouvelle couche Conv2d R√âDUITE\n",
        "            new_conv = nn.Conv2d(\n",
        "                in_channels=new_in_channels,\n",
        "                out_channels=new_out_channels,\n",
        "                kernel_size=layer.kernel_size,\n",
        "                stride=layer.stride,\n",
        "                padding=layer.padding,\n",
        "                bias=(layer.bias is not None)\n",
        "            ).to(device)\n",
        "\n",
        "            # Copier uniquement les poids des filtres conserv√©s\n",
        "            with torch.no_grad():\n",
        "                for new_out_idx, old_out_idx in enumerate(keep_out_channels):\n",
        "                    for new_in_idx, old_in_idx in enumerate(keep_in_channels):\n",
        "                        new_conv.weight[new_out_idx, new_in_idx] = layer.weight[old_out_idx, old_in_idx]\n",
        "\n",
        "                    if layer.bias is not None:\n",
        "                        new_conv.bias[new_out_idx] = layer.bias[old_out_idx]\n",
        "\n",
        "            new_layers.append(new_conv)\n",
        "            prev_kept_channels = keep_out_channels\n",
        "\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            # Adapter BatchNorm aux canaux conserv√©s\n",
        "            if prev_kept_channels is not None:\n",
        "                new_bn = nn.BatchNorm2d(len(prev_kept_channels)).to(device)\n",
        "                with torch.no_grad():\n",
        "                    for new_idx, old_idx in enumerate(prev_kept_channels):\n",
        "                        new_bn.weight[new_idx] = layer.weight[old_idx]\n",
        "                        new_bn.bias[new_idx] = layer.bias[old_idx]\n",
        "                        new_bn.running_mean[new_idx] = layer.running_mean[old_idx]\n",
        "                        new_bn.running_var[new_idx] = layer.running_var[old_idx]\n",
        "                new_layers.append(new_bn)\n",
        "            else:\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            # Ajuster la premi√®re couche Linear apr√®s Flatten\n",
        "            if len([l for l in new_layers if isinstance(l, nn.Linear)]) == 0:\n",
        "                # Calculer la nouvelle taille apr√®s les couches conv\n",
        "                new_input_features = calculate_linear_input_size(new_layers, input_size)\n",
        "\n",
        "                new_linear = nn.Linear(\n",
        "                    in_features=new_input_features,\n",
        "                    out_features=layer.out_features,\n",
        "                    bias=(layer.bias is not None)\n",
        "                ).to(device)\n",
        "\n",
        "                # Copier les poids (avec adaptation de taille)\n",
        "                with torch.no_grad():\n",
        "                    min_features = min(new_input_features, layer.in_features)\n",
        "                    new_linear.weight[:, :min_features] = layer.weight[:, :min_features]\n",
        "                    if layer.bias is not None:\n",
        "                        new_linear.bias[:] = layer.bias[:]\n",
        "\n",
        "                new_layers.append(new_linear)\n",
        "            else:\n",
        "                # Autres couches Linear: copier tel quel\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "        else:\n",
        "            # Autres couches (ReLU, MaxPool, Dropout, Flatten)\n",
        "            new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "    return nn.Sequential(*new_layers).to(device)\n",
        "\n",
        "def calculate_linear_input_size(conv_layers, input_size):\n",
        "    \"\"\"\n",
        "    Calcule la taille d'entr√©e pour la premi√®re couche Linear\n",
        "    en simulant une passe avant √† travers les couches conv.\n",
        "    \"\"\"\n",
        "    channels, h, w = input_size\n",
        "\n",
        "    for layer in conv_layers:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Calculer nouvelle taille apr√®s convolution\n",
        "            kernel_size = layer.kernel_size[0] if isinstance(layer.kernel_size, tuple) else layer.kernel_size\n",
        "            stride = layer.stride[0] if isinstance(layer.stride, tuple) else layer.stride\n",
        "            padding = layer.padding[0] if isinstance(layer.padding, tuple) else layer.padding\n",
        "\n",
        "            h = (h + 2*padding - kernel_size) // stride + 1\n",
        "            w = (w + 2*padding - kernel_size) // stride + 1\n",
        "            channels = layer.out_channels\n",
        "\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            kernel_size = layer.kernel_size\n",
        "            stride = layer.stride if layer.stride else kernel_size\n",
        "            h = h // stride\n",
        "            w = w // stride\n",
        "\n",
        "    return channels * h * w\n",
        "\n",
        "def structured_channel_pruning(model, importance_dict, keep_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Pruning structur√©: supprime des canaux entiers bas√© sur leur importance moyenne.\n",
        "    Plus efficace pour r√©duire r√©ellement les param√®tres.\n",
        "\n",
        "    Args:\n",
        "        model: mod√®le √† pruner\n",
        "        importance_dict: dict[layer][out_ch][in_ch] = importance\n",
        "        keep_ratio: fraction de canaux √† conserver (0.5 = 50%)\n",
        "    \"\"\"\n",
        "    # Calculer l'importance moyenne par canal de sortie\n",
        "    channel_importance = {}\n",
        "\n",
        "    for layer_idx in importance_dict:\n",
        "        channel_importance[layer_idx] = {}\n",
        "        for out_ch in importance_dict[layer_idx]:\n",
        "            # Moyenne d'importance sur tous les canaux d'entr√©e pour ce canal de sortie\n",
        "            avg_importance = np.mean(list(importance_dict[layer_idx][out_ch].values()))\n",
        "            channel_importance[layer_idx][out_ch] = avg_importance\n",
        "\n",
        "    # S√©lectionner les top canaux pour chaque couche\n",
        "    top_indices = []\n",
        "    for layer_idx in channel_importance:\n",
        "        # Trier par importance d√©croissante\n",
        "        channels_by_importance = sorted(\n",
        "            channel_importance[layer_idx].items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=False\n",
        "        )\n",
        "\n",
        "        # Garder le top keep_ratio\n",
        "        n_keep = max(1, int(len(channels_by_importance) * keep_ratio))\n",
        "        top_channels = [ch for ch, _ in channels_by_importance[:n_keep]]\n",
        "\n",
        "        # Cr√©er les indices pour tous les canaux d'entr√©e de ces canaux de sortie\n",
        "        for out_ch in top_channels:\n",
        "            for in_ch in importance_dict[layer_idx][out_ch]:\n",
        "                top_indices.append((layer_idx, out_ch, in_ch))\n",
        "\n",
        "    return top_indices\n",
        "\n",
        "def test_effective_pruning(model, importance_dict, trn_dl, val_dl, loss_fn, keep_ratios=[0.8]):\n",
        "    \"\"\"\n",
        "    Test de pruning avec r√©duction EFFECTIVE des param√®tres.\n",
        "    \"\"\"\n",
        "    print(\"Test de Pruning avec R√©duction Effective des Param√®tres\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Baseline\n",
        "    orig_params = sum(p.numel() for p in model.parameters())\n",
        "    orig_loss, orig_acc = evaluate_model(model, val_dl, loss_fn)\n",
        "\n",
        "    print(f\"Mod√®le original: {orig_acc:.4f} accuracy, {orig_params:,} param√®tres\")\n",
        "    print()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for keep_ratio in keep_ratios:\n",
        "        print(f\"Test avec keep_ratio = {keep_ratio} ({keep_ratio*100:.0f}% des canaux)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Pruning structur√© par canaux\n",
        "            top_indices = structured_channel_pruning(model, importance_dict, keep_ratio)\n",
        "            print(f\"Filtres s√©lectionn√©s: {len(top_indices)}\")\n",
        "\n",
        "            # Appliquer le pruning EFFECTIF\n",
        "            pruned_model = prune_model_by_filters_removal(model, top_indices, input_size=(3, 32, 32))\n",
        "\n",
        "            # V√©rifier la r√©duction\n",
        "            pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
        "            actual_reduction = (orig_params - pruned_params) / orig_params * 100\n",
        "\n",
        "            print(f\"Param√®tres avant: {orig_params:,}\")\n",
        "            print(f\"Param√®tres apr√®s: {pruned_params:,}\")\n",
        "            print(f\"R√©duction R√âELLE: {actual_reduction:.1f}%\")\n",
        "\n",
        "            # Test du mod√®le prun√©\n",
        "            val_loss_before, val_acc_before = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "            print(f\"Accuracy avant fine-tuning: {val_acc_before:.4f}\")\n",
        "\n",
        "            # Fine-tuning\n",
        "            print(\"Fine-tuning...\")\n",
        "            pruned_model = fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=5, lr=1e-3)\n",
        "\n",
        "            val_loss_final, val_acc_final = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "\n",
        "            accuracy_retention = (val_acc_final / orig_acc) * 100\n",
        "            compression_ratio = orig_params / pruned_params\n",
        "\n",
        "            print(f\"Accuracy finale: {val_acc_final:.4f}\")\n",
        "            print(f\"R√©tention accuracy: {accuracy_retention:.1f}%\")\n",
        "            print(f\"Ratio de compression: {compression_ratio:.1f}x\")\n",
        "            print()\n",
        "\n",
        "            results.append({\n",
        "                'keep_ratio': keep_ratio,\n",
        "                'actual_reduction': actual_reduction,\n",
        "                'final_accuracy': val_acc_final,\n",
        "                'accuracy_retention': accuracy_retention,\n",
        "                'compression_ratio': compression_ratio,\n",
        "                'pruned_params': pruned_params\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec keep_ratio {keep_ratio}: {e}\")\n",
        "            print()\n",
        "\n",
        "    # R√©sum√© des r√©sultats\n",
        "    print(\"R√âSUM√â DES R√âSULTATS\")\n",
        "    print(\"=\"*40)\n",
        "    for r in results:\n",
        "        print(f\"Keep {r['keep_ratio']*100:.0f}%: {r['actual_reduction']:.1f}% r√©duction, \"\n",
        "              f\"{r['accuracy_retention']:.1f}% accuracy, {r['compression_ratio']:.1f}x compression\")\n",
        "\n",
        "    return pruned_model\n",
        "\n",
        "# Fonction d'√©valuation (si pas d√©j√† d√©finie)\n",
        "def evaluate_model(model, val_dl, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            device = next(model.parameters()).device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            total_loss += loss_fn(pred, y).item() * x.size(0)\n",
        "            total_correct += (pred.argmax(1) == y).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "def fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=10, lr=1e-4):\n",
        "    \"\"\"Fine-tune le mod√®le prun√© pour r√©cup√©rer l'accuracy.\"\"\"\n",
        "    from torch.optim import Adam\n",
        "\n",
        "    device = next(pruned_model.parameters()).device\n",
        "    optimizer = Adam(pruned_model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pruned_model.train()\n",
        "        for x, y in trn_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = pruned_model(x)\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation check\n",
        "        val_loss, val_acc = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "        if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    return pruned_model\n",
        "\n",
        "# Usage example:\n",
        "# Remplacer votre appel par:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calcul des scores pour chaque m√©thode standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# CALCUL DES SCORES POUR CHAQUE M√âTHODE\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_magnitude_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par magnitude (L1 norm) - baseline classique\n",
        "    Les filtres avec petite norme L1 sont consid√©r√©s moins importants\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par magnitude L1...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "                # Score = norme L1 (somme des valeurs absolues)\n",
        "                score = np.abs(weight).sum()\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_l2_magnitude_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par magnitude L2 (norme euclidienne)\n",
        "    Variante du pruning par magnitude\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par magnitude L2...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "                # Score = norme L2\n",
        "                score = np.sqrt(np.sum(weight**2))\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_variance_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par variance\n",
        "    Les filtres avec faible variance sont consid√©r√©s moins informatifs\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par variance...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "                # Score = variance des poids\n",
        "                score = np.var(weight)\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_random_ranking(model, seed=42):\n",
        "    \"\"\"\n",
        "    Pruning al√©atoire - baseline pour v√©rifier que les autres m√©thodes\n",
        "    font mieux que le hasard\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking al√©atoire...\")\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                # Score al√©atoire entre 0 et 1\n",
        "                score = np.random.random()\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FONCTION DE COMPARAISON GLOBALE\n",
        "# ============================================================================\n",
        "\n",
        "def compare_pruning_methods(model, trn_dl, val_dl, loss_fn,\n",
        "                            filter_ranking_topological=None,\n",
        "                            keep_ratios=[0.8, 0.6, 0.4, 0.2],\n",
        "                            fine_tune_epochs=5):\n",
        "    \"\"\"\n",
        "    Compare diff√©rentes m√©thodes de pruning\n",
        "\n",
        "    Args:\n",
        "        model: mod√®le PyTorch entra√Æn√©\n",
        "        trn_dl, val_dl: dataloaders train/validation\n",
        "        loss_fn: fonction de perte\n",
        "        filter_ranking_topological: votre ranking topologique (optionnel)\n",
        "        keep_ratios: ratios de filtres √† conserver\n",
        "        fine_tune_epochs: nombre d'√©poques de fine-tuning\n",
        "\n",
        "    Returns:\n",
        "        dict avec r√©sultats pour chaque m√©thode\n",
        "    \"\"\"\n",
        "\n",
        "    # Baseline : performance du mod√®le original\n",
        "    print(\"=\"*80)\n",
        "    print(\"√âVALUATION DU MOD√àLE ORIGINAL\")\n",
        "    print(\"=\"*80)\n",
        "    orig_params = sum(p.numel() for p in model.parameters())\n",
        "    orig_loss, orig_acc = evaluate_model(model, val_dl, loss_fn)\n",
        "    print(f\"Accuracy: {orig_acc:.4f}\")\n",
        "    print(f\"Param√®tres: {orig_params:,}\\n\")\n",
        "\n",
        "    # D√©finition des m√©thodes √† tester\n",
        "    methods = {\n",
        "        'L1 Magnitude': calculate_magnitude_ranking,\n",
        "        'L2 Magnitude': calculate_l2_magnitude_ranking,\n",
        "        'Variance': calculate_variance_ranking,\n",
        "        'Random': calculate_random_ranking,\n",
        "    }\n",
        "\n",
        "    if filter_ranking_topological is not None:\n",
        "        methods['Topological (Sobolev)'] = lambda m: filter_ranking_topological\n",
        "\n",
        "    # Stocker tous les r√©sultats\n",
        "    all_results = {}\n",
        "\n",
        "    # Tester chaque m√©thode\n",
        "    for method_name, ranking_function in methods.items():\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"M√âTHODE: {method_name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            # Calculer le ranking (sauf si d√©j√† fourni)\n",
        "            if method_name == 'Topological (Sobolev)' and filter_ranking_topological is not None:\n",
        "                ranking = filter_ranking_topological\n",
        "            else:\n",
        "                ranking = ranking_function(copy.deepcopy(model))\n",
        "\n",
        "            # Tester le pruning avec cette m√©thode\n",
        "            model_copy = copy.deepcopy(model)\n",
        "            results = test_effective_pruning(\n",
        "                model_copy,\n",
        "                ranking,\n",
        "                trn_dl,\n",
        "                val_dl,\n",
        "                loss_fn,\n",
        "                keep_ratios=keep_ratios\n",
        "            )\n",
        "\n",
        "            all_results[method_name] = results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec {method_name}: {e}\")\n",
        "            all_results[method_name] = None\n",
        "\n",
        "    # ========================================================================\n",
        "    # TABLEAU COMPARATIF FINAL\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TABLEAU COMPARATIF - RETENTION D'ACCURACY (%)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Header\n",
        "    header = \"M√©thode\".ljust(25)\n",
        "    for ratio in keep_ratios:\n",
        "        header += f\"Keep {int(ratio*100)}%\".rjust(12)\n",
        "    print(header)\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Lignes pour chaque m√©thode\n",
        "    for method_name in methods.keys():\n",
        "        if all_results[method_name] is None:\n",
        "            continue\n",
        "\n",
        "        line = method_name.ljust(25)\n",
        "        for ratio in keep_ratios:\n",
        "            result = [r for r in all_results[method_name] if r['keep_ratio'] == ratio]\n",
        "            if result:\n",
        "                acc_ret = result[0]['accuracy_retention']\n",
        "                line += f\"{acc_ret:10.1f}%\".rjust(12)\n",
        "            else:\n",
        "                line += \"N/A\".rjust(12)\n",
        "        print(line)\n",
        "\n",
        "    # ========================================================================\n",
        "    # GRAPHIQUE DE COMPARAISON\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSE PAR RATIO DE COMPRESSION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for ratio in keep_ratios:\n",
        "        print(f\"\\n--- Keep {int(ratio*100)}% des filtres ---\")\n",
        "\n",
        "        results_at_ratio = []\n",
        "        for method_name in methods.keys():\n",
        "            if all_results[method_name] is None:\n",
        "                continue\n",
        "            result = [r for r in all_results[method_name] if r['keep_ratio'] == ratio]\n",
        "            if result:\n",
        "                results_at_ratio.append({\n",
        "                    'method': method_name,\n",
        "                    'acc_retention': result[0]['accuracy_retention'],\n",
        "                    'compression': result[0]['compression_ratio']\n",
        "                })\n",
        "\n",
        "        # Trier par accuracy retention\n",
        "        results_at_ratio.sort(key=lambda x: x['acc_retention'], reverse=True)\n",
        "\n",
        "        for i, res in enumerate(results_at_ratio):\n",
        "            rank = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else f\"{i+1}.\"\n",
        "            print(f\"{rank} {res['method']:25s}: {res['acc_retention']:5.1f}% \"\n",
        "                  f\"(compression {res['compression']:.1f}x)\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STATISTIQUES GLOBALES\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STATISTIQUES GLOBALES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for method_name in methods.keys():\n",
        "        if all_results[method_name] is None:\n",
        "            continue\n",
        "\n",
        "        accs = [r['accuracy_retention'] for r in all_results[method_name]]\n",
        "        print(f\"\\n{method_name}:\")\n",
        "        print(f\"  Moyenne: {np.mean(accs):.1f}%\")\n",
        "        print(f\"  Min: {np.min(accs):.1f}%\")\n",
        "        print(f\"  Max: {np.max(accs):.1f}%\")\n",
        "        print(f\"  √âcart-type: {np.std(accs):.1f}%\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FONCTION HELPER POUR √âVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, val_dl, loss_fn):\n",
        "    \"\"\"√âvalue le mod√®le sur le dataset de validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            total_loss += loss_fn(pred, y).item() * x.size(0)\n",
        "            total_correct += (pred.argmax(1) == y).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Charger votre mod√®le entra√Æn√© et vos dataloaders\n",
        "    # model, trn_dl, val_dl, loss_fn = ...\n",
        "\n",
        "    # Si vous avez d√©j√† calcul√© votre ranking topologique\n",
        "    # filter_ranking_topological = ...\n",
        "\n",
        "    # Lancer la comparaison\n",
        "    results = compare_pruning_methods(\n",
        "        model=model,\n",
        "        trn_dl=trn_dl,\n",
        "        val_dl=val_dl,\n",
        "        loss_fn=loss_fn,\n",
        "        filter_ranking_topological=filter_ranking,  # Votre m√©thode\n",
        "        keep_ratios=[0.8, 0.6, 0.4, 0.2],\n",
        "        fine_tune_epochs=5\n",
        "    )\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
