{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP697LuRwBgo",
        "outputId": "542f2151-dd7b-46ad-81e2-651849115d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "import torch\n",
        "data_folder = '/content/' # This can be any directory you want to download FMNIST to\n",
        "fmnist = datasets.CIFAR10(data_folder, download=True, train=True)\n",
        "\n",
        "tr_images = fmnist.data\n",
        "tr_targets = fmnist.targets\n",
        "\n",
        "val_fmnist = datasets.CIFAR10(data_folder, download=True, train=False)\n",
        "val_images = val_fmnist.data\n",
        "val_targets = val_fmnist.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2cuyLuLPwCeA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, targets, device='cpu'):\n",
        "        # Convertir les tableaux NumPy en tenseurs PyTorch\n",
        "        images = torch.from_numpy(images).float() / 255.0\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        # CIFAR10 : images en 3x32x32, format (N, H, W, C) -> (N, C, H, W)\n",
        "        images = images.permute(0, 3, 1, 2)\n",
        "\n",
        "        self.x = images\n",
        "        self.y = targets\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        x = self.x[ix].to(self.device)\n",
        "        y = self.y[ix].to(self.device)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BHGn69pmwFCk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_model(device='cpu'):\n",
        "    model = nn.Sequential(\n",
        "        # Bloc 1\n",
        "        nn.Conv2d(3, 16, kernel_size=5, padding=2, bias=True),  # 64 → 16\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(16),\n",
        "\n",
        "        nn.Conv2d(16, 16, kernel_size=5, padding=2, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(0.1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.MaxPool2d(2),  # 32x32 → 16x16\n",
        "\n",
        "        # Bloc 2\n",
        "        nn.Conv2d(16, 32, kernel_size=5, padding=2, bias=True),  # 128 → 32\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(0.1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(2),  # 16x16 → 8x8\n",
        "\n",
        "        # Bloc 3\n",
        "        nn.Conv2d(32, 32, kernel_size=5, padding=2, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(0.1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(2),  # 8x8 → 4x4\n",
        "\n",
        "        # Classificateur\n",
        "        nn.Flatten(),              # 32 * 4 * 4 = 512\n",
        "        nn.Linear(512, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(64, 10)\n",
        "    ).to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    return model, loss_fn, optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F8B-MtQwIrx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oABLdS0NwJQx"
      },
      "outputs": [],
      "source": [
        "def get_data():\n",
        "    train = CIFAR10Dataset(tr_images, tr_targets)\n",
        "    trn_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    val = CIFAR10Dataset(val_images, val_targets)\n",
        "    val_dl = DataLoader(val, batch_size=len(val_images), shuffle=True)\n",
        "    return trn_dl, val_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGA7dXVLwLO5",
        "outputId": "eb3dc5f0-1301-4149-ff3a-e2cef219ddc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]           1,216\n",
            "              ReLU-2           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-3           [-1, 16, 32, 32]              32\n",
            "            Conv2d-4           [-1, 16, 32, 32]           6,416\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "         Dropout2d-6           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "         MaxPool2d-8           [-1, 16, 16, 16]               0\n",
            "            Conv2d-9           [-1, 32, 16, 16]          12,832\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "        Dropout2d-11           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "        MaxPool2d-13             [-1, 32, 8, 8]               0\n",
            "           Conv2d-14             [-1, 32, 8, 8]          25,632\n",
            "             ReLU-15             [-1, 32, 8, 8]               0\n",
            "        Dropout2d-16             [-1, 32, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 32, 8, 8]              64\n",
            "        MaxPool2d-18             [-1, 32, 4, 4]               0\n",
            "          Flatten-19                  [-1, 512]               0\n",
            "           Linear-20                  [-1, 128]          65,664\n",
            "             ReLU-21                  [-1, 128]               0\n",
            "          Dropout-22                  [-1, 128]               0\n",
            "           Linear-23                   [-1, 64]           8,256\n",
            "             ReLU-24                   [-1, 64]               0\n",
            "          Dropout-25                   [-1, 64]               0\n",
            "           Linear-26                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 120,858\n",
            "Trainable params: 120,858\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.25\n",
            "Params size (MB): 0.46\n",
            "Estimated Total Size (MB): 1.72\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Définir le device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Charger les données\n",
        "trn_dl, val_dl = get_data()\n",
        "\n",
        "# Initialiser le modèle\n",
        "model, loss_fn, optimizer = get_model(device=device)\n",
        "\n",
        "# Afficher un résumé du modèle\n",
        "from torchsummary import summary\n",
        "summary(model, (3, 32, 32))  # CIFAR10 : 3 canaux, 32x32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c43c5VK8wOVy"
      },
      "outputs": [],
      "source": [
        "def train_batch(x, y, model, opt, loss_fn):\n",
        "    prediction = model(x)\n",
        "    batch_loss = loss_fn(prediction, y)\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return batch_loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(x, y, model):\n",
        "    model.eval()\n",
        "    prediction = model(x)\n",
        "    max_values, argmaxes = prediction.max(-1)\n",
        "    is_correct = argmaxes == y\n",
        "    return is_correct.cpu().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N8s7IsyUwR-Q"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def val_loss(x, y, model):\n",
        "    model.eval()\n",
        "    prediction = model(x)\n",
        "    val_loss = loss_fn(prediction, y)\n",
        "    return val_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-blw4yXLwTnb",
        "outputId": "3d5ef518-43b3-4021-b92f-3227a8036390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "for epoch in range(6):\n",
        "    print(epoch)\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "        train_epoch_losses.append(batch_loss)\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        is_correct = accuracy(x, y, model)\n",
        "        train_epoch_accuracies.extend(is_correct)\n",
        "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
        "\n",
        "    for ix, batch in enumerate(iter(val_dl)):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        val_is_correct = accuracy(x, y, model)\n",
        "        validation_loss = val_loss(x, y, model)\n",
        "    val_epoch_accuracy = np.mean(val_is_correct)\n",
        "\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    train_accuracies.append(train_epoch_accuracy)\n",
        "    val_losses.append(validation_loss)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvxloP_xpeU"
      },
      "source": [
        "**INTERPOLATION DU FILTRE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kGYfASukxtuA"
      },
      "outputs": [],
      "source": [
        "def interpolate_filter(filter_weights, smooth_factor=50):\n",
        "    \"\"\"Interpolation cubique bilinéaire pour passer du filtre discret à une fonction continue\"\"\"\n",
        "    h, w = filter_weights.shape\n",
        "    y = np.arange(h)\n",
        "    x = np.arange(w)\n",
        "    interp_func = RegularGridInterpolator((y, x), filter_weights, method='cubic')\n",
        "    ynew = np.linspace(0, h-1, smooth_factor)\n",
        "    xnew = np.linspace(0, w-1, smooth_factor)\n",
        "    X, Y = np.meshgrid(xnew, ynew)\n",
        "    points = np.array([Y.ravel(), X.ravel()]).T\n",
        "    Zsmooth = interp_func(points).reshape(smooth_factor, smooth_factor)\n",
        "    return Zsmooth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI3eqSuFxubf"
      },
      "source": [
        "**Projection dans schwartz**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xA0CEZ2Vx3AF"
      },
      "outputs": [],
      "source": [
        "def project_to_schwartz(Zsmooth, sigma=None):\n",
        "    \"\"\"Projette un filtre interpolé dans un espace proche de S(R^2) via une fenêtre gaussienne\"\"\"\n",
        "    H, W = Zsmooth.shape\n",
        "    if sigma is None:\n",
        "        sigma = min(H, W) / 2\n",
        "    y = np.arange(H) - H/2\n",
        "    x = np.arange(W) - W/2\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    gauss_window = np.exp(-(X**2 + Y**2) / (2*sigma**2))\n",
        "    Z_schwartz = Zsmooth * gauss_window\n",
        "    return Z_schwartz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8bfJzHt1uFQ"
      },
      "source": [
        "**Mesure de régularité**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eFS613_Wx7NN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def gaussian_window(shape, sigma=1.0, center=None):\n",
        "    \"\"\"Fenêtre gaussienne normalisée 2D\"\"\"\n",
        "    n, m = shape\n",
        "    if center is None:\n",
        "        center = (n//2, m//2)\n",
        "    y = np.arange(n) - center[0]\n",
        "    x = np.arange(m) - center[1]\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    g = np.exp(-(X**2 + Y**2)/(2*sigma**2))\n",
        "    g /= np.sqrt(np.sum(g**2))\n",
        "    return g\n",
        "\n",
        "def local_sobolev_gauss_3D(h, block_size=(5,5), alpha=2, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Calcule la régularité locale de sous-blocs d'un filtre 2D\n",
        "    avec fenêtre gaussienne et pondération Sobolev.\n",
        "\n",
        "    Retour :\n",
        "    - reg_map : matrice de régularité locale\n",
        "    \"\"\"\n",
        "    n, m = h.shape\n",
        "    bh, bw = block_size\n",
        "    reg_map = np.zeros((n - bh + 1, m - bw + 1))\n",
        "\n",
        "    for i in range(n - bh + 1):\n",
        "        for j in range(m - bw + 1):\n",
        "            block = h[i:i+bh, j:j+bw]\n",
        "            g = gaussian_window(block.shape, sigma=sigma)\n",
        "            block_win = block * g\n",
        "\n",
        "            H = np.fft.fft2(block_win)\n",
        "            H = np.fft.fftshift(H)\n",
        "\n",
        "            u = np.fft.fftshift(np.fft.fftfreq(bh))\n",
        "            v = np.fft.fftshift(np.fft.fftfreq(bw))\n",
        "            U, V = np.meshgrid(u, v, indexing='ij')\n",
        "\n",
        "            freq_weight = (1 + U**2 + V**2)**alpha\n",
        "            reg_map[i,j] = np.sum(freq_weight * np.abs(H)**2)\n",
        "\n",
        "    return reg_map\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_7Fds61Q10"
      },
      "source": [
        "**Récupération des poids réguliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "90sq5XIUxeBg"
      },
      "outputs": [],
      "source": [
        "def graphcut_filter_surface(Zsmooth, percentile=20):\n",
        "    \"\"\"\n",
        "    Conserve uniquement les valeurs les plus petites (en valeur absolue)\n",
        "    selon un seuil basé sur le percentile.\n",
        "    #\n",
        "    Zsmooth : surface lissée (2D numpy array)\n",
        "    percentile : pourcentage de valeurs à conserver (les plus petites)\n",
        "    \"\"\"\n",
        "    threshold = np.percentile(np.abs(Zsmooth), percentile)\n",
        "    mask = np.abs(Zsmooth) >= threshold  # True là où on garde\n",
        "    Zcut = np.where(mask, Zsmooth, np.nan)  # NaN pour visualiser les trous\n",
        "    return Zcut, mask\n",
        "\n",
        "# Application sur reg_map\n",
        "#Zcut, mask = graphcut_filter_surface(reg_map, percentile=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjFLTC0414E0"
      },
      "source": [
        "Mesure de valeur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0ky5RZdD13Sm"
      },
      "outputs": [],
      "source": [
        "def magnitude_mesure(Zcut):\n",
        "    \"\"\"\n",
        "    Calcule la somme des valeurs absolues de Zcut.\n",
        "    Les NaN sont traités comme des zéros.\n",
        "    \"\"\"\n",
        "    return np.nansum(np.abs(Zcut))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PkpoBSxo8Ef6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "def ultimate_graphcut_filter_analysis_magn(filter_weights):\n",
        "  Zsmooth = interpolate_filter(filter_weights)\n",
        "  Z_schwartz = project_to_schwartz(Zsmooth)\n",
        "  reg_map = local_sobolev_gauss_3D(Zsmooth, block_size=(5,5), alpha=2, sigma=1.0)\n",
        "  Zcut, mask = graphcut_filter_surface(reg_map, percentile=80)\n",
        "  mask = mask.astype(np.float32)\n",
        "  mask = cv2.resize(mask, (50, 50), interpolation=cv2.INTER_LINEAR)\n",
        "  mask = mask > 0.5  # Re-seuillage\n",
        "  Zcut_schwartz = np.where(mask, Z_schwartz, np.nan)\n",
        "  return magnitude_mesure(Zcut_schwartz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp20qDf12KYW"
      },
      "source": [
        "**Classement des filtres**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgwbKDtt6rmd",
        "outputId": "1b38080e-b515-4bb4-ac13-8a7b5fd4a46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Couche 1/4...\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "1\n",
            "2\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "0\n",
            "1\n",
            "2\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "5\n",
            "0\n",
            "1\n",
            "2\n",
            "6\n",
            "0\n",
            "1\n",
            "2\n",
            "7\n",
            "0\n",
            "1\n",
            "2\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "10\n",
            "0\n",
            "1\n",
            "2\n",
            "11\n",
            "0\n",
            "1\n",
            "2\n",
            "12\n",
            "0\n",
            "1\n",
            "2\n",
            "13\n",
            "0\n",
            "1\n",
            "2\n",
            "14\n",
            "0\n",
            "1\n",
            "2\n",
            "15\n",
            "0\n",
            "1\n",
            "2\n",
            "   Couche 2/4...\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "3\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "5\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "6\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "7\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "10\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "11\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "12\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "13\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "14\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "15\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "   Couche 3/4...\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "3\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "5\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "6\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "7\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "10\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "11\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "12\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "13\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "14\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "15\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "17\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "18\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "19\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "20\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "21\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "22\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "23\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "25\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "26\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "27\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "28\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "29\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "30\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "31\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "   Couche 4/4...\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "3\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "5\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "6\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "7\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "10\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "11\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "12\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "13\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "14\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "15\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "16\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "17\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "18\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "19\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "20\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "21\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "22\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "23\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "25\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "26\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "27\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "28\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "29\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "30\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "31\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n"
          ]
        }
      ],
      "source": [
        "from scipy.interpolate import RegularGridInterpolator\n",
        "conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n",
        "\n",
        "filter_ranking = {}\n",
        "conv_layers = [m for m in model.modules() if isinstance(m, torch.nn.Conv2d)]\n",
        "\n",
        "for m_idx, conv_layer in enumerate(conv_layers):\n",
        "    print(f\"   Couche {m_idx+1}/{len(conv_layers)}...\")\n",
        "    filter_ranking[m_idx] = {}\n",
        "\n",
        "    out_channels, in_channels, h, w = conv_layer.weight.shape\n",
        "\n",
        "    for oc in range(out_channels):\n",
        "        print(oc)\n",
        "        filter_ranking[m_idx][oc] = {}\n",
        "        for ic in range(in_channels):\n",
        "            print(ic)\n",
        "                # 1. Importance GraphCut multi-échelle + structurelle\n",
        "            filter_weights = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "            base_importance = ultimate_graphcut_filter_analysis_magn(filter_weights)\n",
        "\n",
        "                # 2. Correction contextuelle (redondance)\n",
        "            #redundancy_factor = contextual_redundancy_check(conv_layer, oc, ic)\n",
        "\n",
        "                # 3. Score final\n",
        "            #final_importance = base_importance * redundancy_factor\n",
        "            filter_ranking[m_idx][oc][ic] = float(base_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjCKJdwqyTtc"
      },
      "outputs": [],
      "source": [
        "filter_ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EYv_f--Fr1hi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_filter_ranking_combined(filter_ranking):\n",
        "    \"\"\"\n",
        "    Affiche tous les filtres de toutes les couches dans un seul graphe,\n",
        "    avec une ligne rouge séparant les couches.\n",
        "\n",
        "    Args:\n",
        "        filter_ranking : dict[layer_idx][out_ch][in_ch] = score\n",
        "    \"\"\"\n",
        "    all_scores = []\n",
        "    layer_boundaries = []\n",
        "    current_idx = 0\n",
        "\n",
        "    # Parcourir les couches\n",
        "    for l_idx in sorted(filter_ranking.keys()):\n",
        "        layer_dict = filter_ranking[l_idx]\n",
        "        # Moyenne sur les canaux d'entrée pour chaque filtre de sortie\n",
        "        out_scores = [np.mean(list(layer_dict[oc].values())) for oc in sorted(layer_dict.keys())]\n",
        "        all_scores.extend(out_scores)\n",
        "        current_idx += len(out_scores)\n",
        "        layer_boundaries.append(current_idx)  # position pour la ligne rouge\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.bar(range(len(all_scores)), all_scores)\n",
        "    plt.xlabel(\"Filtres (toutes couches concaténées)\")\n",
        "    plt.ylabel(\"Score d'importance moyen\")\n",
        "    plt.title(\"Importance des filtres par couche\")\n",
        "\n",
        "    # Tracer les lignes rouges pour séparer les couches\n",
        "    for b in layer_boundaries[:-1]:  # pas besoin de la dernière\n",
        "        plt.axvline(x=b-0.5, color='red', linestyle='--', linewidth=1)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRTxF81tr4bU"
      },
      "outputs": [],
      "source": [
        "plot_filter_ranking_combined(filter_ranking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1n4xHA7AqbN"
      },
      "outputs": [],
      "source": [
        "plot_filter_ranking_combined(filter_ranking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXAz5Fg9entf"
      },
      "outputs": [],
      "source": [
        "filter_ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vxw1vTDg15WY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b3cdb996-7cc8-40e8-89b8-5ddf56e86576"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6bddf8b7-006e-4626-85e7-d1f618761f2b\", \"reg_magn_80.json\", 70075)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "with open(\"reg_magn_80.json\", \"w\") as f:\n",
        "    json.dump(filter_ranking, f, indent=4)\n",
        "from google.colab import files\n",
        "files.download(\"reg_magn_80.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QtoD_SR1FFG7"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "model_test = copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtB4MCV6FXgU"
      },
      "source": [
        "**Sélection des filtres**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g5lUEDEW9J1V"
      },
      "outputs": [],
      "source": [
        "def keep_top_percentile_filters(filter_ranking, percentile):\n",
        "    \"\"\"\n",
        "    Keeps the top 'percentile' of filters based on their base_importance score.\n",
        "\n",
        "    Args:\n",
        "        filter_ranking (dict): A dictionary containing the ranking of filters.\n",
        "                               Expected format: {layer_idx: {out_channel_idx: {in_channel_idx: base_importance}}}\n",
        "        percentile (int): The percentile to keep (e.g., 10 for top 10%).\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary containing only the top percentile filters.\n",
        "    \"\"\"\n",
        "    all_importances = []\n",
        "    # Collect all base_importance values\n",
        "    for layer_idx, out_channels in filter_ranking.items():\n",
        "        for out_channel_idx, in_channels in out_channels.items():\n",
        "            for in_channel_idx, importance in in_channels.items():\n",
        "                # Only consider non-NaN importance values\n",
        "                if not np.isnan(importance):\n",
        "                    all_importances.append(importance)\n",
        "\n",
        "    if not all_importances:\n",
        "        print(\"No valid importance values found.\")\n",
        "        return {}\n",
        "\n",
        "    # Calculate the threshold based on the percentile\n",
        "    threshold = np.percentile(all_importances, 100 - percentile) # Keep values >= threshold\n",
        "\n",
        "    # Create a new dictionary with only the top percentile filters\n",
        "    top_filters_ranking = {}\n",
        "    for layer_idx, out_channels in filter_ranking.items():\n",
        "        top_filters_ranking[layer_idx] = {}\n",
        "        for out_channel_idx, in_channels in out_channels.items():\n",
        "            top_filters_ranking[layer_idx][out_channel_idx] = {}\n",
        "            for in_channel_idx, importance in in_channels.items():\n",
        "                if not np.isnan(importance) and importance >= threshold:\n",
        "                    top_filters_ranking[layer_idx][out_channel_idx][in_channel_idx] = importance\n",
        "\n",
        "    return top_filters_ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1pVbhwylbrF"
      },
      "outputs": [],
      "source": [
        "top_list = keep_top_percentile_filters(filter_ranking,40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJg-VzejFbTq"
      },
      "source": [
        "**Pruning du modèle*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cf8xk3vXFuog"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def prune_model_by_filters_removal(model, top_indices_list, input_size=(3, 32, 32)):\n",
        "    \"\"\"\n",
        "    VERSION CORRIGÉE: Supprime réellement les filtres pour réduire les paramètres.\n",
        "\n",
        "    Args:\n",
        "        model: modèle PyTorch nn.Sequential\n",
        "        top_indices_list: [(layer_idx, out_ch, in_ch), ...] - filtres à CONSERVER\n",
        "        input_size: taille d'entrée (channels, height, width)\n",
        "\n",
        "    Returns:\n",
        "        Modèle pruné avec moins de paramètres\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model_copy = copy.deepcopy(model)\n",
        "\n",
        "    # Organiser les filtres à conserver par couche\n",
        "    filters_to_keep = defaultdict(set)\n",
        "    for layer_idx, out_ch, in_ch in top_indices_list:\n",
        "        filters_to_keep[layer_idx].add(out_ch)\n",
        "\n",
        "    # Convertir en listes triées\n",
        "    for layer_idx in filters_to_keep:\n",
        "        filters_to_keep[layer_idx] = sorted(list(filters_to_keep[layer_idx]))\n",
        "\n",
        "    new_layers = []\n",
        "    conv_layers = [i for i, layer in enumerate(model_copy) if isinstance(layer, nn.Conv2d)]\n",
        "    prev_kept_channels = None\n",
        "\n",
        "    for i, layer in enumerate(model_copy):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Trouver l'index de cette couche conv\n",
        "            conv_idx = conv_layers.index(i)\n",
        "\n",
        "            # Déterminer les canaux de sortie à conserver\n",
        "            if conv_idx in filters_to_keep:\n",
        "                keep_out_channels = filters_to_keep[conv_idx]\n",
        "            else:\n",
        "                # Si pas spécifié, garder tous les canaux\n",
        "                keep_out_channels = list(range(layer.out_channels))\n",
        "\n",
        "            # Déterminer les canaux d'entrée\n",
        "            if conv_idx == 0:\n",
        "                # Première couche: garder tous les canaux d'entrée (RGB)\n",
        "                new_in_channels = layer.in_channels\n",
        "                keep_in_channels = list(range(layer.in_channels))\n",
        "            else:\n",
        "                # Couches suivantes: s'adapter aux canaux conservés de la couche précédente\n",
        "                new_in_channels = len(prev_kept_channels) if prev_kept_channels else layer.in_channels\n",
        "                keep_in_channels = prev_kept_channels if prev_kept_channels else list(range(layer.in_channels))\n",
        "\n",
        "            new_out_channels = len(keep_out_channels)\n",
        "\n",
        "            # Créer la nouvelle couche Conv2d RÉDUITE\n",
        "            new_conv = nn.Conv2d(\n",
        "                in_channels=new_in_channels,\n",
        "                out_channels=new_out_channels,\n",
        "                kernel_size=layer.kernel_size,\n",
        "                stride=layer.stride,\n",
        "                padding=layer.padding,\n",
        "                bias=(layer.bias is not None)\n",
        "            ).to(device)\n",
        "\n",
        "            # Copier uniquement les poids des filtres conservés\n",
        "            with torch.no_grad():\n",
        "                for new_out_idx, old_out_idx in enumerate(keep_out_channels):\n",
        "                    for new_in_idx, old_in_idx in enumerate(keep_in_channels):\n",
        "                        new_conv.weight[new_out_idx, new_in_idx] = layer.weight[old_out_idx, old_in_idx]\n",
        "\n",
        "                    if layer.bias is not None:\n",
        "                        new_conv.bias[new_out_idx] = layer.bias[old_out_idx]\n",
        "\n",
        "            new_layers.append(new_conv)\n",
        "            prev_kept_channels = keep_out_channels\n",
        "\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            # Adapter BatchNorm aux canaux conservés\n",
        "            if prev_kept_channels is not None:\n",
        "                new_bn = nn.BatchNorm2d(len(prev_kept_channels)).to(device)\n",
        "                with torch.no_grad():\n",
        "                    for new_idx, old_idx in enumerate(prev_kept_channels):\n",
        "                        new_bn.weight[new_idx] = layer.weight[old_idx]\n",
        "                        new_bn.bias[new_idx] = layer.bias[old_idx]\n",
        "                        new_bn.running_mean[new_idx] = layer.running_mean[old_idx]\n",
        "                        new_bn.running_var[new_idx] = layer.running_var[old_idx]\n",
        "                new_layers.append(new_bn)\n",
        "            else:\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            # Ajuster la première couche Linear après Flatten\n",
        "            if len([l for l in new_layers if isinstance(l, nn.Linear)]) == 0:\n",
        "                # Calculer la nouvelle taille après les couches conv\n",
        "                new_input_features = calculate_linear_input_size(new_layers, input_size)\n",
        "\n",
        "                new_linear = nn.Linear(\n",
        "                    in_features=new_input_features,\n",
        "                    out_features=layer.out_features,\n",
        "                    bias=(layer.bias is not None)\n",
        "                ).to(device)\n",
        "\n",
        "                # Copier les poids (avec adaptation de taille)\n",
        "                with torch.no_grad():\n",
        "                    min_features = min(new_input_features, layer.in_features)\n",
        "                    new_linear.weight[:, :min_features] = layer.weight[:, :min_features]\n",
        "                    if layer.bias is not None:\n",
        "                        new_linear.bias[:] = layer.bias[:]\n",
        "\n",
        "                new_layers.append(new_linear)\n",
        "            else:\n",
        "                # Autres couches Linear: copier tel quel\n",
        "                new_layers.append(copy.deepcopy(layer))\n",
        "        else:\n",
        "            # Autres couches (ReLU, MaxPool, Dropout, Flatten)\n",
        "            new_layers.append(copy.deepcopy(layer))\n",
        "\n",
        "    return nn.Sequential(*new_layers).to(device)\n",
        "\n",
        "def calculate_linear_input_size(conv_layers, input_size):\n",
        "    \"\"\"\n",
        "    Calcule la taille d'entrée pour la première couche Linear\n",
        "    en simulant une passe avant à travers les couches conv.\n",
        "    \"\"\"\n",
        "    channels, h, w = input_size\n",
        "\n",
        "    for layer in conv_layers:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Calculer nouvelle taille après convolution\n",
        "            kernel_size = layer.kernel_size[0] if isinstance(layer.kernel_size, tuple) else layer.kernel_size\n",
        "            stride = layer.stride[0] if isinstance(layer.stride, tuple) else layer.stride\n",
        "            padding = layer.padding[0] if isinstance(layer.padding, tuple) else layer.padding\n",
        "\n",
        "            h = (h + 2*padding - kernel_size) // stride + 1\n",
        "            w = (w + 2*padding - kernel_size) // stride + 1\n",
        "            channels = layer.out_channels\n",
        "\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            kernel_size = layer.kernel_size\n",
        "            stride = layer.stride if layer.stride else kernel_size\n",
        "            h = h // stride\n",
        "            w = w // stride\n",
        "\n",
        "    return channels * h * w\n",
        "\n",
        "def structured_channel_pruning(model, importance_dict, keep_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Pruning structuré: supprime des canaux entiers basé sur leur importance moyenne.\n",
        "    Plus efficace pour réduire réellement les paramètres.\n",
        "\n",
        "    Args:\n",
        "        model: modèle à pruner\n",
        "        importance_dict: dict[layer][out_ch][in_ch] = importance\n",
        "        keep_ratio: fraction de canaux à conserver (0.5 = 50%)\n",
        "    \"\"\"\n",
        "    # Calculer l'importance moyenne par canal de sortie\n",
        "    channel_importance = {}\n",
        "\n",
        "    for layer_idx in importance_dict:\n",
        "        channel_importance[layer_idx] = {}\n",
        "        for out_ch in importance_dict[layer_idx]:\n",
        "            # Moyenne d'importance sur tous les canaux d'entrée pour ce canal de sortie\n",
        "            avg_importance = np.mean(list(importance_dict[layer_idx][out_ch].values()))\n",
        "            channel_importance[layer_idx][out_ch] = avg_importance\n",
        "\n",
        "    # Sélectionner les top canaux pour chaque couche\n",
        "    top_indices = []\n",
        "    for layer_idx in channel_importance:\n",
        "        # Trier par importance décroissante\n",
        "        channels_by_importance = sorted(\n",
        "            channel_importance[layer_idx].items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Garder le top keep_ratio\n",
        "        n_keep = max(1, int(len(channels_by_importance) * keep_ratio))\n",
        "        top_channels = [ch for ch, _ in channels_by_importance[:n_keep]]\n",
        "\n",
        "        # Créer les indices pour tous les canaux d'entrée de ces canaux de sortie\n",
        "        for out_ch in top_channels:\n",
        "            for in_ch in importance_dict[layer_idx][out_ch]:\n",
        "                top_indices.append((layer_idx, out_ch, in_ch))\n",
        "\n",
        "    return top_indices\n",
        "\n",
        "def test_effective_pruning(model, importance_dict, trn_dl, val_dl, loss_fn, keep_ratios=[0.8, 0.6, 0.4, 0.2]):\n",
        "    \"\"\"\n",
        "    Test de pruning avec réduction EFFECTIVE des paramètres.\n",
        "    \"\"\"\n",
        "    print(\"Test de Pruning avec Réduction Effective des Paramètres\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Baseline\n",
        "    orig_params = sum(p.numel() for p in model.parameters())\n",
        "    orig_loss, orig_acc = evaluate_model(model, val_dl, loss_fn)\n",
        "\n",
        "    print(f\"Modèle original: {orig_acc:.4f} accuracy, {orig_params:,} paramètres\")\n",
        "    print()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for keep_ratio in keep_ratios:\n",
        "        print(f\"Test avec keep_ratio = {keep_ratio} ({keep_ratio*100:.0f}% des canaux)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Pruning structuré par canaux\n",
        "            top_indices = structured_channel_pruning(model, importance_dict, keep_ratio)\n",
        "            print(f\"Filtres sélectionnés: {len(top_indices)}\")\n",
        "\n",
        "            # Appliquer le pruning EFFECTIF\n",
        "            pruned_model = prune_model_by_filters_removal(model, top_indices, input_size=(3, 32, 32))\n",
        "\n",
        "            # Vérifier la réduction\n",
        "            pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
        "            actual_reduction = (orig_params - pruned_params) / orig_params * 100\n",
        "\n",
        "            print(f\"Paramètres avant: {orig_params:,}\")\n",
        "            print(f\"Paramètres après: {pruned_params:,}\")\n",
        "            print(f\"Réduction RÉELLE: {actual_reduction:.1f}%\")\n",
        "\n",
        "            # Test du modèle pruné\n",
        "            val_loss_before, val_acc_before = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "            print(f\"Accuracy avant fine-tuning: {val_acc_before:.4f}\")\n",
        "\n",
        "            # Fine-tuning\n",
        "            print(\"Fine-tuning...\")\n",
        "            pruned_model = fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=5, lr=1e-3)\n",
        "\n",
        "            val_loss_final, val_acc_final = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "\n",
        "            accuracy_retention = (val_acc_final / orig_acc) * 100\n",
        "            compression_ratio = orig_params / pruned_params\n",
        "\n",
        "            print(f\"Accuracy finale: {val_acc_final:.4f}\")\n",
        "            print(f\"Rétention accuracy: {accuracy_retention:.1f}%\")\n",
        "            print(f\"Ratio de compression: {compression_ratio:.1f}x\")\n",
        "            print()\n",
        "\n",
        "            results.append({\n",
        "                'keep_ratio': keep_ratio,\n",
        "                'actual_reduction': actual_reduction,\n",
        "                'final_accuracy': val_acc_final,\n",
        "                'accuracy_retention': accuracy_retention,\n",
        "                'compression_ratio': compression_ratio,\n",
        "                'pruned_params': pruned_params\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec keep_ratio {keep_ratio}: {e}\")\n",
        "            print()\n",
        "\n",
        "    # Résumé des résultats\n",
        "    print(\"RÉSUMÉ DES RÉSULTATS\")\n",
        "    print(\"=\"*40)\n",
        "    for r in results:\n",
        "        print(f\"Keep {r['keep_ratio']*100:.0f}%: {r['actual_reduction']:.1f}% réduction, \"\n",
        "              f\"{r['accuracy_retention']:.1f}% accuracy, {r['compression_ratio']:.1f}x compression\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Fonction d'évaluation (si pas déjà définie)\n",
        "def evaluate_model(model, val_dl, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            device = next(model.parameters()).device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            total_loss += loss_fn(pred, y).item() * x.size(0)\n",
        "            total_correct += (pred.argmax(1) == y).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "def fine_tune_pruned_model(pruned_model, trn_dl, val_dl, loss_fn, epochs=10, lr=1e-4):\n",
        "    \"\"\"Fine-tune le modèle pruné pour récupérer l'accuracy.\"\"\"\n",
        "    from torch.optim import Adam\n",
        "\n",
        "    device = next(pruned_model.parameters()).device\n",
        "    optimizer = Adam(pruned_model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pruned_model.train()\n",
        "        for x, y in trn_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = pruned_model(x)\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation check\n",
        "        val_loss, val_acc = evaluate_model(pruned_model, val_dl, loss_fn)\n",
        "        if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    return pruned_model\n",
        "\n",
        "# Usage example:\n",
        "# Remplacer votre appel par:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xyvjgo6gajdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b66bbd-6805-4754-9758-aabc8c79864a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.0710\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6584\n",
            "  Epoch 2: Val Acc = 0.6994\n",
            "  Epoch 4: Val Acc = 0.7182\n",
            "Accuracy finale: 0.7227\n",
            "Rétention accuracy: 97.9%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.0698\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6473\n",
            "  Epoch 2: Val Acc = 0.6665\n",
            "  Epoch 4: Val Acc = 0.6991\n",
            "Accuracy finale: 0.7089\n",
            "Rétention accuracy: 96.0%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.0861\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5760\n",
            "  Epoch 2: Val Acc = 0.6019\n",
            "  Epoch 4: Val Acc = 0.6332\n",
            "Accuracy finale: 0.6548\n",
            "Rétention accuracy: 88.7%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.1135\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4602\n",
            "  Epoch 2: Val Acc = 0.4958\n",
            "  Epoch 4: Val Acc = 0.5470\n",
            "Accuracy finale: 0.5487\n",
            "Rétention accuracy: 74.3%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 97.9% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 96.0% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 88.7% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 74.3% accuracy, 5.2x compression\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'keep_ratio': 0.8,\n",
              "  'actual_reduction': 27.120256830329808,\n",
              "  'final_accuracy': 0.7227,\n",
              "  'accuracy_retention': 97.86052809749492,\n",
              "  'compression_ratio': 1.372123386428401,\n",
              "  'pruned_params': 88081},\n",
              " {'keep_ratio': 0.6,\n",
              "  'actual_reduction': 46.950967250823275,\n",
              "  'final_accuracy': 0.7089,\n",
              "  'accuracy_retention': 95.99187542315504,\n",
              "  'compression_ratio': 1.8850485073462895,\n",
              "  'pruned_params': 64114},\n",
              " {'keep_ratio': 0.4,\n",
              "  'actual_reduction': 66.51607671813203,\n",
              "  'final_accuracy': 0.6548,\n",
              "  'accuracy_retention': 88.66621530128639,\n",
              "  'compression_ratio': 2.98650785806069,\n",
              "  'pruned_params': 40468},\n",
              " {'keep_ratio': 0.2,\n",
              "  'actual_reduction': 80.82377666352248,\n",
              "  'final_accuracy': 0.5487,\n",
              "  'accuracy_retention': 74.29925524712253,\n",
              "  'compression_ratio': 5.214791163272351,\n",
              "  'pruned_params': 23176}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "test_effective_pruning(model_test,filter_ranking,trn_dl, val_dl, loss_fn) #gardant 50 plus régulière"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70nA-O9NAxG2"
      },
      "outputs": [],
      "source": [
        "test_effective_pruning(model_test,filter_ranking,trn_dl, val_dl, loss_fn) #gardant 20 plus régulière"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# CALCUL DES SCORES POUR CHAQUE MÉTHODE\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_magnitude_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par magnitude (L1 norm) - baseline classique\n",
        "    Les filtres avec petite norme L1 sont considérés moins importants\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par magnitude L1...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "                # Score = norme L1 (somme des valeurs absolues)\n",
        "                score = np.abs(weight).sum()\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_l2_magnitude_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par magnitude L2 (norme euclidienne)\n",
        "    Variante du pruning par magnitude\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par magnitude L2...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "                # Score = norme L2\n",
        "                score = np.sqrt(np.sum(weight**2))\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_variance_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par variance\n",
        "    Les filtres avec faible variance sont considérés moins informatifs\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par variance...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "                # Score = variance des poids\n",
        "                score = np.var(weight)\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_random_ranking(model, seed=42):\n",
        "    \"\"\"\n",
        "    Pruning aléatoire - baseline pour vérifier que les autres méthodes\n",
        "    font mieux que le hasard\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking aléatoire...\")\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                # Score aléatoire entre 0 et 1\n",
        "                score = np.random.random()\n",
        "                filter_ranking[m_idx][oc][ic] = score\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "def calculate_entropy_ranking(model):\n",
        "    \"\"\"\n",
        "    Pruning par entropie - mesure de la distribution des poids\n",
        "    Filtres avec faible entropie (poids similaires) sont moins informatifs\n",
        "    \"\"\"\n",
        "    print(\"Calcul du ranking par entropie...\")\n",
        "    filter_ranking = {}\n",
        "    conv_layers = [m for m in model.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    for m_idx, conv_layer in enumerate(conv_layers):\n",
        "        filter_ranking[m_idx] = {}\n",
        "        out_channels, in_channels = conv_layer.weight.shape[:2]\n",
        "\n",
        "        for oc in range(out_channels):\n",
        "            filter_ranking[m_idx][oc] = {}\n",
        "            for ic in range(in_channels):\n",
        "                weight = conv_layer.weight[oc, ic].detach().cpu().numpy()\n",
        "\n",
        "                # Normaliser pour avoir une distribution\n",
        "                weight_abs = np.abs(weight.flatten())\n",
        "                weight_norm = weight_abs / (weight_abs.sum() + 1e-10)\n",
        "\n",
        "                # Calculer l'entropie\n",
        "                entropy = -np.sum(weight_norm * np.log(weight_norm + 1e-10))\n",
        "                filter_ranking[m_idx][oc][ic] = entropy\n",
        "\n",
        "    return filter_ranking\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FONCTION DE COMPARAISON GLOBALE\n",
        "# ============================================================================\n",
        "\n",
        "def compare_pruning_methods(model, trn_dl, val_dl, loss_fn,\n",
        "                            filter_ranking_topological=None,\n",
        "                            keep_ratios=[0.8, 0.6, 0.4, 0.2],\n",
        "                            fine_tune_epochs=5):\n",
        "    \"\"\"\n",
        "    Compare différentes méthodes de pruning\n",
        "\n",
        "    Args:\n",
        "        model: modèle PyTorch entraîné\n",
        "        trn_dl, val_dl: dataloaders train/validation\n",
        "        loss_fn: fonction de perte\n",
        "        filter_ranking_topological: votre ranking topologique (optionnel)\n",
        "        keep_ratios: ratios de filtres à conserver\n",
        "        fine_tune_epochs: nombre d'époques de fine-tuning\n",
        "\n",
        "    Returns:\n",
        "        dict avec résultats pour chaque méthode\n",
        "    \"\"\"\n",
        "\n",
        "    # Baseline : performance du modèle original\n",
        "    print(\"=\"*80)\n",
        "    print(\"ÉVALUATION DU MODÈLE ORIGINAL\")\n",
        "    print(\"=\"*80)\n",
        "    orig_params = sum(p.numel() for p in model.parameters())\n",
        "    orig_loss, orig_acc = evaluate_model(model, val_dl, loss_fn)\n",
        "    print(f\"Accuracy: {orig_acc:.4f}\")\n",
        "    print(f\"Paramètres: {orig_params:,}\\n\")\n",
        "\n",
        "    # Définir les méthodes à tester\n",
        "    methods = {\n",
        "        'L1 Magnitude': calculate_magnitude_ranking,\n",
        "        'L2 Magnitude': calculate_l2_magnitude_ranking,\n",
        "        'Variance': calculate_variance_ranking,\n",
        "        'Entropy': calculate_entropy_ranking,\n",
        "        'Random': calculate_random_ranking,\n",
        "    }\n",
        "\n",
        "    if filter_ranking_topological is not None:\n",
        "        methods['Topological (Sobolev)'] = lambda m: filter_ranking_topological\n",
        "\n",
        "    # Stocker tous les résultats\n",
        "    all_results = {}\n",
        "\n",
        "    # Tester chaque méthode\n",
        "    for method_name, ranking_function in methods.items():\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"MÉTHODE: {method_name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            # Calculer le ranking (sauf si déjà fourni)\n",
        "            if method_name == 'Topological (Sobolev)' and filter_ranking_topological is not None:\n",
        "                ranking = filter_ranking_topological\n",
        "            else:\n",
        "                ranking = ranking_function(copy.deepcopy(model))\n",
        "\n",
        "            # Tester le pruning avec cette méthode\n",
        "            model_copy = copy.deepcopy(model)\n",
        "            results = test_effective_pruning(\n",
        "                model_copy,\n",
        "                ranking,\n",
        "                trn_dl,\n",
        "                val_dl,\n",
        "                loss_fn,\n",
        "                keep_ratios=keep_ratios\n",
        "            )\n",
        "\n",
        "            all_results[method_name] = results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec {method_name}: {e}\")\n",
        "            all_results[method_name] = None\n",
        "\n",
        "    # ========================================================================\n",
        "    # TABLEAU COMPARATIF FINAL\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TABLEAU COMPARATIF - RETENTION D'ACCURACY (%)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Header\n",
        "    header = \"Méthode\".ljust(25)\n",
        "    for ratio in keep_ratios:\n",
        "        header += f\"Keep {int(ratio*100)}%\".rjust(12)\n",
        "    print(header)\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Lignes pour chaque méthode\n",
        "    for method_name in methods.keys():\n",
        "        if all_results[method_name] is None:\n",
        "            continue\n",
        "\n",
        "        line = method_name.ljust(25)\n",
        "        for ratio in keep_ratios:\n",
        "            result = [r for r in all_results[method_name] if r['keep_ratio'] == ratio]\n",
        "            if result:\n",
        "                acc_ret = result[0]['accuracy_retention']\n",
        "                line += f\"{acc_ret:10.1f}%\".rjust(12)\n",
        "            else:\n",
        "                line += \"N/A\".rjust(12)\n",
        "        print(line)\n",
        "\n",
        "    # ========================================================================\n",
        "    # GRAPHIQUE DE COMPARAISON\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSE PAR RATIO DE COMPRESSION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for ratio in keep_ratios:\n",
        "        print(f\"\\n--- Keep {int(ratio*100)}% des filtres ---\")\n",
        "\n",
        "        results_at_ratio = []\n",
        "        for method_name in methods.keys():\n",
        "            if all_results[method_name] is None:\n",
        "                continue\n",
        "            result = [r for r in all_results[method_name] if r['keep_ratio'] == ratio]\n",
        "            if result:\n",
        "                results_at_ratio.append({\n",
        "                    'method': method_name,\n",
        "                    'acc_retention': result[0]['accuracy_retention'],\n",
        "                    'compression': result[0]['compression_ratio']\n",
        "                })\n",
        "\n",
        "        # Trier par accuracy retention\n",
        "        results_at_ratio.sort(key=lambda x: x['acc_retention'], reverse=True)\n",
        "\n",
        "        for i, res in enumerate(results_at_ratio):\n",
        "            rank = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else f\"{i+1}.\"\n",
        "            print(f\"{rank} {res['method']:25s}: {res['acc_retention']:5.1f}% \"\n",
        "                  f\"(compression {res['compression']:.1f}x)\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STATISTIQUES GLOBALES\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STATISTIQUES GLOBALES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for method_name in methods.keys():\n",
        "        if all_results[method_name] is None:\n",
        "            continue\n",
        "\n",
        "        accs = [r['accuracy_retention'] for r in all_results[method_name]]\n",
        "        print(f\"\\n{method_name}:\")\n",
        "        print(f\"  Moyenne: {np.mean(accs):.1f}%\")\n",
        "        print(f\"  Min: {np.min(accs):.1f}%\")\n",
        "        print(f\"  Max: {np.max(accs):.1f}%\")\n",
        "        print(f\"  Écart-type: {np.std(accs):.1f}%\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FONCTION HELPER POUR ÉVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, val_dl, loss_fn):\n",
        "    \"\"\"Évalue le modèle sur le dataset de validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_dl:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            total_loss += loss_fn(pred, y).item() * x.size(0)\n",
        "            total_correct += (pred.argmax(1) == y).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXEMPLE D'UTILISATION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Charger votre modèle entraîné et vos dataloaders\n",
        "    # model, trn_dl, val_dl, loss_fn = ...\n",
        "\n",
        "    # Si vous avez déjà calculé votre ranking topologique\n",
        "    # filter_ranking_topological = ...\n",
        "\n",
        "    # Lancer la comparaison\n",
        "    results = compare_pruning_methods(\n",
        "        model=model,\n",
        "        trn_dl=trn_dl,\n",
        "        val_dl=val_dl,\n",
        "        loss_fn=loss_fn,\n",
        "        filter_ranking_topological=filter_ranking,  # Votre méthode\n",
        "        keep_ratios=[0.8, 0.6, 0.4, 0.2],\n",
        "        fine_tune_epochs=5\n",
        "    )\n",
        "\n",
        "    # Les résultats sont dans 'results'\n",
        "    # Format: results[method_name] = liste de dicts avec accuracy_retention, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACKUl1LCm-KE",
        "outputId": "d65522c4-db69-46f1-c228-5a30e19f9c7b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ÉVALUATION DU MODÈLE ORIGINAL\n",
            "================================================================================\n",
            "Accuracy: 0.7385\n",
            "Paramètres: 120,858\n",
            "\n",
            "\n",
            "================================================================================\n",
            "MÉTHODE: L1 Magnitude\n",
            "================================================================================\n",
            "Calcul du ranking par magnitude L1...\n",
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.0701\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6680\n",
            "  Epoch 2: Val Acc = 0.6988\n",
            "  Epoch 4: Val Acc = 0.7264\n",
            "Accuracy finale: 0.7219\n",
            "Rétention accuracy: 97.8%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.0720\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6324\n",
            "  Epoch 2: Val Acc = 0.6737\n",
            "  Epoch 4: Val Acc = 0.6940\n",
            "Accuracy finale: 0.7020\n",
            "Rétention accuracy: 95.1%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.1015\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5642\n",
            "  Epoch 2: Val Acc = 0.6027\n",
            "  Epoch 4: Val Acc = 0.6294\n",
            "Accuracy finale: 0.6419\n",
            "Rétention accuracy: 86.9%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.1161\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4594\n",
            "  Epoch 2: Val Acc = 0.4984\n",
            "  Epoch 4: Val Acc = 0.5376\n",
            "Accuracy finale: 0.5400\n",
            "Rétention accuracy: 73.1%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 97.8% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 95.1% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 86.9% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 73.1% accuracy, 5.2x compression\n",
            "\n",
            "================================================================================\n",
            "MÉTHODE: L2 Magnitude\n",
            "================================================================================\n",
            "Calcul du ranking par magnitude L2...\n",
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.1239\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6722\n",
            "  Epoch 2: Val Acc = 0.7047\n",
            "  Epoch 4: Val Acc = 0.7298\n",
            "Accuracy finale: 0.7248\n",
            "Rétention accuracy: 98.1%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.0810\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6217\n",
            "  Epoch 2: Val Acc = 0.6576\n",
            "  Epoch 4: Val Acc = 0.6879\n",
            "Accuracy finale: 0.7032\n",
            "Rétention accuracy: 95.2%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.0985\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5590\n",
            "  Epoch 2: Val Acc = 0.5925\n",
            "  Epoch 4: Val Acc = 0.6266\n",
            "Accuracy finale: 0.6355\n",
            "Rétention accuracy: 86.1%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.1077\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4438\n",
            "  Epoch 2: Val Acc = 0.4945\n",
            "  Epoch 4: Val Acc = 0.5247\n",
            "Accuracy finale: 0.5459\n",
            "Rétention accuracy: 73.9%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 98.1% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 95.2% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 86.1% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 73.9% accuracy, 5.2x compression\n",
            "\n",
            "================================================================================\n",
            "MÉTHODE: Variance\n",
            "================================================================================\n",
            "Calcul du ranking par variance...\n",
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.1043\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6658\n",
            "  Epoch 2: Val Acc = 0.6998\n",
            "  Epoch 4: Val Acc = 0.7215\n",
            "Accuracy finale: 0.7252\n",
            "Rétention accuracy: 98.2%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.0983\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6461\n",
            "  Epoch 2: Val Acc = 0.6670\n",
            "  Epoch 4: Val Acc = 0.6908\n",
            "Accuracy finale: 0.6947\n",
            "Rétention accuracy: 94.1%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.1395\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5628\n",
            "  Epoch 2: Val Acc = 0.5864\n",
            "  Epoch 4: Val Acc = 0.6457\n",
            "Accuracy finale: 0.6493\n",
            "Rétention accuracy: 87.9%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.0987\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4385\n",
            "  Epoch 2: Val Acc = 0.5123\n",
            "  Epoch 4: Val Acc = 0.5539\n",
            "Accuracy finale: 0.5659\n",
            "Rétention accuracy: 76.6%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 98.2% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 94.1% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 87.9% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 76.6% accuracy, 5.2x compression\n",
            "\n",
            "================================================================================\n",
            "MÉTHODE: Entropy\n",
            "================================================================================\n",
            "Calcul du ranking par entropie...\n",
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.1042\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6571\n",
            "  Epoch 2: Val Acc = 0.6884\n",
            "  Epoch 4: Val Acc = 0.7177\n",
            "Accuracy finale: 0.7267\n",
            "Rétention accuracy: 98.4%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.0921\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5839\n",
            "  Epoch 2: Val Acc = 0.6305\n",
            "  Epoch 4: Val Acc = 0.6686\n",
            "Accuracy finale: 0.6826\n",
            "Rétention accuracy: 92.4%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.1019\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4757\n",
            "  Epoch 2: Val Acc = 0.5426\n",
            "  Epoch 4: Val Acc = 0.5954\n",
            "Accuracy finale: 0.6140\n",
            "Rétention accuracy: 83.1%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.1056\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4167\n",
            "  Epoch 2: Val Acc = 0.4471\n",
            "  Epoch 4: Val Acc = 0.5084\n",
            "Accuracy finale: 0.5018\n",
            "Rétention accuracy: 67.9%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 98.4% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 92.4% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 83.1% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 67.9% accuracy, 5.2x compression\n",
            "\n",
            "================================================================================\n",
            "MÉTHODE: Random\n",
            "================================================================================\n",
            "Calcul du ranking aléatoire...\n",
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.0941\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6586\n",
            "  Epoch 2: Val Acc = 0.6956\n",
            "  Epoch 4: Val Acc = 0.7169\n",
            "Accuracy finale: 0.7175\n",
            "Rétention accuracy: 97.2%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.1010\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6000\n",
            "  Epoch 2: Val Acc = 0.6348\n",
            "  Epoch 4: Val Acc = 0.6850\n",
            "Accuracy finale: 0.6877\n",
            "Rétention accuracy: 93.1%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.1115\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5290\n",
            "  Epoch 2: Val Acc = 0.5632\n",
            "  Epoch 4: Val Acc = 0.6101\n",
            "Accuracy finale: 0.6249\n",
            "Rétention accuracy: 84.6%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.1000\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4013\n",
            "  Epoch 2: Val Acc = 0.4474\n",
            "  Epoch 4: Val Acc = 0.4829\n",
            "Accuracy finale: 0.5038\n",
            "Rétention accuracy: 68.2%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 97.2% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 93.1% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 84.6% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 68.2% accuracy, 5.2x compression\n",
            "\n",
            "================================================================================\n",
            "MÉTHODE: Topological (Sobolev)\n",
            "================================================================================\n",
            "Test de Pruning avec Réduction Effective des Paramètres\n",
            "============================================================\n",
            "Modèle original: 0.7385 accuracy, 120,858 paramètres\n",
            "\n",
            "Test avec keep_ratio = 0.8 (80% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1428\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 88,081\n",
            "Réduction RÉELLE: 27.1%\n",
            "Accuracy avant fine-tuning: 0.0710\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6691\n",
            "  Epoch 2: Val Acc = 0.7010\n",
            "  Epoch 4: Val Acc = 0.7149\n",
            "Accuracy finale: 0.7247\n",
            "Rétention accuracy: 98.1%\n",
            "Ratio de compression: 1.4x\n",
            "\n",
            "Test avec keep_ratio = 0.6 (60% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 1083\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 64,114\n",
            "Réduction RÉELLE: 47.0%\n",
            "Accuracy avant fine-tuning: 0.0698\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.6413\n",
            "  Epoch 2: Val Acc = 0.6646\n",
            "  Epoch 4: Val Acc = 0.6784\n",
            "Accuracy finale: 0.6922\n",
            "Rétention accuracy: 93.7%\n",
            "Ratio de compression: 1.9x\n",
            "\n",
            "Test avec keep_ratio = 0.4 (40% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 690\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 40,468\n",
            "Réduction RÉELLE: 66.5%\n",
            "Accuracy avant fine-tuning: 0.0861\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.5738\n",
            "  Epoch 2: Val Acc = 0.6154\n",
            "  Epoch 4: Val Acc = 0.6484\n",
            "Accuracy finale: 0.6548\n",
            "Rétention accuracy: 88.7%\n",
            "Ratio de compression: 3.0x\n",
            "\n",
            "Test avec keep_ratio = 0.2 (20% des canaux)\n",
            "----------------------------------------\n",
            "Filtres sélectionnés: 345\n",
            "Paramètres avant: 120,858\n",
            "Paramètres après: 23,176\n",
            "Réduction RÉELLE: 80.8%\n",
            "Accuracy avant fine-tuning: 0.1135\n",
            "Fine-tuning...\n",
            "  Epoch 1: Val Acc = 0.4476\n",
            "  Epoch 2: Val Acc = 0.5108\n",
            "  Epoch 4: Val Acc = 0.5430\n",
            "Accuracy finale: 0.5420\n",
            "Rétention accuracy: 73.4%\n",
            "Ratio de compression: 5.2x\n",
            "\n",
            "RÉSUMÉ DES RÉSULTATS\n",
            "========================================\n",
            "Keep 80%: 27.1% réduction, 98.1% accuracy, 1.4x compression\n",
            "Keep 60%: 47.0% réduction, 93.7% accuracy, 1.9x compression\n",
            "Keep 40%: 66.5% réduction, 88.7% accuracy, 3.0x compression\n",
            "Keep 20%: 80.8% réduction, 73.4% accuracy, 5.2x compression\n",
            "\n",
            "================================================================================\n",
            "TABLEAU COMPARATIF - RETENTION D'ACCURACY (%)\n",
            "================================================================================\n",
            "Méthode                      Keep 80%    Keep 60%    Keep 40%    Keep 20%\n",
            "--------------------------------------------------------------------------------\n",
            "L1 Magnitude                    97.8%       95.1%       86.9%       73.1%\n",
            "L2 Magnitude                    98.1%       95.2%       86.1%       73.9%\n",
            "Variance                        98.2%       94.1%       87.9%       76.6%\n",
            "Entropy                         98.4%       92.4%       83.1%       67.9%\n",
            "Random                          97.2%       93.1%       84.6%       68.2%\n",
            "Topological (Sobolev)           98.1%       93.7%       88.7%       73.4%\n",
            "\n",
            "================================================================================\n",
            "ANALYSE PAR RATIO DE COMPRESSION\n",
            "================================================================================\n",
            "\n",
            "--- Keep 80% des filtres ---\n",
            "🥇 Entropy                  :  98.4% (compression 1.4x)\n",
            "🥈 Variance                 :  98.2% (compression 1.4x)\n",
            "🥉 L2 Magnitude             :  98.1% (compression 1.4x)\n",
            "4. Topological (Sobolev)    :  98.1% (compression 1.4x)\n",
            "5. L1 Magnitude             :  97.8% (compression 1.4x)\n",
            "6. Random                   :  97.2% (compression 1.4x)\n",
            "\n",
            "--- Keep 60% des filtres ---\n",
            "🥇 L2 Magnitude             :  95.2% (compression 1.9x)\n",
            "🥈 L1 Magnitude             :  95.1% (compression 1.9x)\n",
            "🥉 Variance                 :  94.1% (compression 1.9x)\n",
            "4. Topological (Sobolev)    :  93.7% (compression 1.9x)\n",
            "5. Random                   :  93.1% (compression 1.9x)\n",
            "6. Entropy                  :  92.4% (compression 1.9x)\n",
            "\n",
            "--- Keep 40% des filtres ---\n",
            "🥇 Topological (Sobolev)    :  88.7% (compression 3.0x)\n",
            "🥈 Variance                 :  87.9% (compression 3.0x)\n",
            "🥉 L1 Magnitude             :  86.9% (compression 3.0x)\n",
            "4. L2 Magnitude             :  86.1% (compression 3.0x)\n",
            "5. Random                   :  84.6% (compression 3.0x)\n",
            "6. Entropy                  :  83.1% (compression 3.0x)\n",
            "\n",
            "--- Keep 20% des filtres ---\n",
            "🥇 Variance                 :  76.6% (compression 5.2x)\n",
            "🥈 L2 Magnitude             :  73.9% (compression 5.2x)\n",
            "🥉 Topological (Sobolev)    :  73.4% (compression 5.2x)\n",
            "4. L1 Magnitude             :  73.1% (compression 5.2x)\n",
            "5. Random                   :  68.2% (compression 5.2x)\n",
            "6. Entropy                  :  67.9% (compression 5.2x)\n",
            "\n",
            "================================================================================\n",
            "STATISTIQUES GLOBALES\n",
            "================================================================================\n",
            "\n",
            "L1 Magnitude:\n",
            "  Moyenne: 88.2%\n",
            "  Min: 73.1%\n",
            "  Max: 97.8%\n",
            "  Écart-type: 9.6%\n",
            "\n",
            "L2 Magnitude:\n",
            "  Moyenne: 88.3%\n",
            "  Min: 73.9%\n",
            "  Max: 98.1%\n",
            "  Écart-type: 9.4%\n",
            "\n",
            "Variance:\n",
            "  Moyenne: 89.2%\n",
            "  Min: 76.6%\n",
            "  Max: 98.2%\n",
            "  Écart-type: 8.1%\n",
            "\n",
            "Entropy:\n",
            "  Moyenne: 85.5%\n",
            "  Min: 67.9%\n",
            "  Max: 98.4%\n",
            "  Écart-type: 11.5%\n",
            "\n",
            "Random:\n",
            "  Moyenne: 85.8%\n",
            "  Min: 68.2%\n",
            "  Max: 97.2%\n",
            "  Écart-type: 11.1%\n",
            "\n",
            "Topological (Sobolev):\n",
            "  Moyenne: 88.5%\n",
            "  Min: 73.4%\n",
            "  Max: 98.1%\n",
            "  Écart-type: 9.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dans votre notebook, après avoir calculé filter_ranking\n",
        "model_test = copy.deepcopy(model)\n",
        "\n",
        "results = compare_pruning_methods(\n",
        "    model=model_test,\n",
        "    trn_dl=trn_dl,\n",
        "    val_dl=val_dl,\n",
        "    loss_fn=loss_fn,\n",
        "    filter_ranking_topological=filter_ranking,  # Votre méthode Sobolev\n",
        "    keep_ratios=[0.8, 0.6, 0.4, 0.2],\n",
        "    fine_tune_epochs=5\n",
        ")"
      ],
      "metadata": {
        "id": "E15ZRXoRnAYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}